{"cells":[{"cell_type":"markdown","source":["# Data Engineering Process\n\nOverall data engineering process for extracting relevant features for machine learning. This includes both variables specific to the flights dataset, and weather variables.\n\n![data_engineering](https://user-images.githubusercontent.com/14703336/101406370-5b74b080-38a7-11eb-8789-e06ecec8294c.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e1af662-75e1-420f-ae83-10344ed13e6a"}}},{"cell_type":"code","source":["# Prepare libraries\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.types import StructType, StructField, StringType, FloatType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType, ArrayType\nfrom pyspark.sql import SQLContext\nimport math\nimport numpy as np\nfrom pyspark.sql.functions import expr,lit,current_timestamp, to_date\n\nsqlContext = SQLContext(sc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3ca7c85-067f-4f30-ac80-e14dbc314084"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# 1. Preparing flights data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e5d07b4-1453-492a-a861-34bd8d2eee99"}}},{"cell_type":"markdown","source":["## Step 1.1: Load data into table\n### Tables: us_delay_flights_tbl / stations_tbl / weather_tbl / airport\n\nFirst, we load the provided data into tables:\n - flights\n - weather\n - stations \n \nIn addition, we load a table for additional airport data, including geographic location (latitude, longitude), IATA code, and the time zone of the airport. We will use this information in merging the weather table with the flights table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f536bc90-ae3a-4616-b99f-76bc91489deb"}}},{"cell_type":"code","source":["spark.sql(\"drop table if exists us_delay_flights_tbl\")\nspark.sql(\"drop table if exists weather_tbl\")\nspark.sql(\"drop table if exists stations_tbl\")\nspark.sql(\"drop table if exists airport_tbl\")\n\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/weather_tbl\", True)\n\nairlines = spark.read.option(\"header\", \"true\").parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project/parquet_airlines_data/201*.parquet\")\nairlines.write.saveAsTable(\"us_delay_flights_tbl\")\n\nweather = spark.read.option(\"header\", \"true\")\\\n                    .parquet(f\"dbfs:/mnt/mids-w261/datasets_final_project/weather_data/*.parquet\")\nstations = spark.read.option(\"header\", \"true\").csv(\"dbfs:/mnt/mids-w261/datasets_final_project/stations/stations.csv.gz\")\n# 28 minutes\nweather.write.saveAsTable(\"weather_tbl\")\nstations.write.saveAsTable(\"stations_tbl\")\n\n# Download airport geo data from openflights: https://openflights.org/data.html\nimport urllib.request\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports-extended.dat\",\"/tmp/airports.dat\")\ndbutils.fs.mv(\"file:/tmp/airports.dat\", \"dbfs:/data/airports.dat\")\n\n#Create schema for the airport table\nairport_schema = StructType([\\\n    StructField(\"airport_id\", IntegerType(), False),\\\n    StructField(\"name\", StringType(), False),\\\n    StructField(\"city\", StringType(), True),\\\n    StructField(\"country\", StringType(), True),\\\n    StructField(\"IATA\", StringType(), False),\\\n    StructField(\"ICAO\", StringType(), True),\\\n    StructField(\"latitude\", DoubleType(), True),\\\n    StructField(\"longitude\", DoubleType(), True),\\\n    StructField(\"altitude\", DoubleType(), True),\\\n    StructField(\"timzone\", StringType(), True),\\\n    StructField(\"DST\", StringType(), True),\\\n    StructField(\"tztimezone\", StringType(), True),\\\n    StructField(\"type\", StringType(), True),\\\n    StructField(\"source\", StringType(), True)])\n\nairportsDF = spark.read.format(\"csv\")\\\n          .option(\"header\", \"false\")\\\n          .schema(airport_schema)\\\n          .load(\"/data/airports.dat\")\n\nairportsDF.write.saveAsTable(\"airport_tbl\")  # airlines geo\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22e0d05e-8dfc-4094-af2a-4dfbf0016c8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Step 1.2: Adding PageRank feature to airports \n### Table: airport_pagerank\n\nAs a potential feature for our model, we calculate the PageRank score for each airport. We think that delays should be related to the centrality of the airport. More central airports likely have more flights going in and out, which can either mean more delays on general, or better regulation and manpower (leading to less delays). If an airport has a flight to another airport, that constitutes a directed link between the two airports. The \"graphframes\" library takes as input a set of vertices (airports), and links (origin and destination for each row) in order to calculate the PageRank score for each airport."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d891f3a-040b-4d98-a734-e6e2d6665ab3"}}},{"cell_type":"code","source":["#1.28 minutes\n# Get PageRank for each airport\nspark.sql(\"drop table if exists airport_pagerank\")\nfrom graphframes import *\nvertices = spark.sql(\"select ORIGIN as id from airport_used_tbl\")\nedges = spark.sql(\"select ORIGIN as src, DEST as dst, DEP_DELAY from us_delay_flights_tbl\")\ng = GraphFrame(vertices, edges)\nranks = g.pageRank(resetProbability=0.15, maxIter=5)\nranks.vertices.orderBy(ranks.vertices.pagerank.desc()).write.saveAsTable(\"airport_pagerank\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f61bda48-008a-45c8-8c69-f13009c52304"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Step 1.3: Add flight delay ratio feature \n### Tables: airport_used_tbl, airport_delay_analysis\n\nThe first feature we have is the flight delay ratio based on the origin of the flight. We categorize a flight into delayed (by at least 15 minutes), non-delayed, and missing (likely a cancellation)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"807bc407-0ddf-4740-b08e-64d1f194be3b"}}},{"cell_type":"code","source":["spark.sql(\"drop table if exists airport_used_tbl\")\nairports_usedDF = spark.sql(\"with airports as (select  ORIGIN from us_delay_flights_tbl union all select DEST from us_delay_flights_tbl) select distinct ORIGIN from airports \")\nairports_usedDF.write.saveAsTable('airport_used_tbl')  # airlines\n\nspark.sql(\"drop table if exists airport_delay_analysis \")\nairport_delay_analysisDF = spark.sql(\"select distinct origin \\\n,sum(dep_del15) as num_delay \\\n,sum(1-dep_del15) as num_non_delay \\\n,sum(if(dep_del15 is null, 1, 0)) as num_missing \\\n,count(*) as total_flights \\\n,sum(dep_del15) / count(*) as ratio \\\n,sum(arr_del15) / count(*) as arr_ratio \\\nfrom us_delay_flights_tbl \\\ngroup by origin \\\norder by ratio\")\nairport_delay_analysisDF.write.saveAsTable(\"airport_delay_analysis\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74795f89-b59b-4368-8393-18ed6392aeaf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Step 1.4: Parse and clean flights data\n### Table: flight_adjusted\n\nWhile we aim to prepare as many variables as possible, we decided that the DIV variables have too many NULL values and cannot be used in modeling. These variables represent \"Diverted Airport\" variables, which are airports capable of handling events such an emergency landing. While potentially useful since emergencies will clearly be prioritized over regular takeoff, these events are rare enough that we often do not see a meaningful value for these variables. As a result, they will not be processed and dropped from the table.\n\nNext, we also calculate the departure time - 2 hrs, representing the ideal time when we would like to merge corresponding weather data. This information is stored at the next merge step."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5588e336-13ee-4b67-8063-bb2f71420cc9"}}},{"cell_type":"code","source":["#create time table flight_adjusted\nspark.sql('drop table if exists flight_adjusted')\ndbutils.fs.rm('/delta/flight_adjusted',True)\n\n#drop irrelevant tables\nairlines\\\n.drop(\"DIV_AIRPORT_LANDINGS\",\"DIV_REACHED_DEST\",\"DIV_ACTUAL_ELAPSED_TIME\",\"DIV_ARR_DELAY\",\"DIV_DISTANCE\",\"DIV1_AIRPORT\")\\\n.drop(\"DIV1_AIRPORT_ID\",\"DIV1_AIRPORT_SEQ_ID\",\"DIV1_WHEELS_ON\",\"DIV1_TOTAL_GTIME\",\"DIV1_LONGEST_GTIME\")\\\n.drop(\"DIV1_WHEELS_OFF\",\"DIV1_TAIL_NUM\",\"DIV2_AIRPORT\",\"DIV2_AIRPORT_ID\",\"DIV2_AIRPORT_SEQ_ID\",\"DIV2_WHEELS_ON\",\"DIV2_TOTAL_GTIME\",\"DIV2_LONGEST_GTIME\")\\\n.drop(\"DIV2_WHEELS_OFF\",\"DIV2_TAIL_NUM\",\"DIV3_AIRPORT\",\"DIV3_AIRPORT_ID\",\"DIV3_AIRPORT_SEQ_ID\",\"DIV3_WHEELS_ON\",\"DIV3_TOTAL_GTIME\",\"DIV3_LONGEST_GTIME\")\\\n.drop(\"DIV3_WHEELS_OFF\",\"DIV3_TAIL_NUM\",\"DIV4_AIRPORT\",\"DIV4_AIRPORT_ID\",\"DIV4_AIRPORT_SEQ_ID\",\"DIV4_WHEELS_ON\",\"DIV4_TOTAL_GTIME\",\"DIV4_LONGEST_GTIME\")\\\n.drop(\"DIV4_WHEELS_OFF\",\"DIV4_TAIL_NUM\",\"DIV5_AIRPORT\",\"DIV5_AIRPORT_ID\",\"DIV5_AIRPORT_SEQ_ID\",\"DIV5_WHEELS_ON\",\"DIV5_TOTAL_GTIME\",\"DIV5_LONGEST_GTIME\")\\\n.drop(\"DIV5_WHEELS_OFF\",\"DIV5_TAIL_NUM\")\\\n.withColumn('departure_time',expr(\"cast(concat(FL_DATE,' ', left(CRS_DEP_TIME,  length(CRS_DEP_TIME)-2), ':',right(CRS_DEP_TIME,2)) as timestamp)\"))\\\n.withColumn('report_time',expr(\"cast(concat(FL_DATE,' ', left(CRS_DEP_TIME,  length(CRS_DEP_TIME)-2), ':',right(CRS_DEP_TIME,2)) as timestamp)- INTERVAL 2 hours\"))\\\n.withColumn('report_time_utc',expr(\"timestamp 'today'\"))\\\n.withColumn('arr_time_dest',expr(\"cast(concat(FL_DATE,' ', left(CRS_ARR_TIME,  length(CRS_ARR_TIME)-2), ':',right(CRS_ARR_TIME,2)) as timestamp)\"))\\\n.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"ORIGIN\").save(\"/delta/flight_adjusted/\")\n\nspark.sql(\"CREATE TABLE flight_adjusted USING DELTA LOCATION '/delta/flight_adjusted/'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54c03d5c-11e2-4656-93df-c14093431191"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Here we merge the flights table with the airport table. In most cases, the ORIGIN from the flights table matches the IATA code from the airports table. There are two exceptions, which is airport marked XWA, which corresponds to IATA code ISN (ISN was decommissioned and renamed XWA). Another airport is TKI, which corresponds to KTKI."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1215074d-49bf-4581-9343-bbeb7321d91e"}}},{"cell_type":"code","source":["%sql\n-- This convers local time to UTC \nMERGE INTO flight_adjusted as f \nUSING airport_tbl AS a \nON f.ORIGIN = a.IATA \nWHEN MATCHED THEN UPDATE set f.report_time_utc = cast(int(f.report_time) - a.timzone * 60 * 60 as timestamp);\n\nMERGE INTO flight_adjusted as f \nUSING airport_tbl AS a \nON f.ORIGIN = 'XWA' and a.IATA ='ISN'\nWHEN MATCHED THEN UPDATE set f.report_time_utc = cast(int(f.report_time) - a.timzone * 60 * 60 as timestamp);\n\nMERGE INTO flight_adjusted as f \nUSING airport_tbl AS a \nON f.ORIGIN = 'TKI' and a.ICAO ='KTKI'\nWHEN MATCHED THEN UPDATE set f.report_time_utc = cast(int(f.report_time) - a.timzone * 60 * 60 as timestamp);"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4190779a-4743-44e6-a2b3-1e7e8134d0da"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Step 1.5: Adding additional feature -- dep_delay_last_hour\n### Tables: flights_delay, flights_add_features, flights_pagerank\n\nIn this section, we calculate for previous hour's delay ratio for each flight, and the ratio of last hour's delay compared to regional average departure delay."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d9f4a93-dcf2-4863-a693-82829c82d053"}}},{"cell_type":"code","source":["# 2.14 minutes\nspark.sql(\"drop table if exists flights_delay\")\nflightDF = spark.sql(\"select *\\\n, ifnull(sum(DEP_DEL15) over (partition by ORIGIN order by cast(departure_time as int) range between 10800 preceding and 7200 preceding)/ count(1) over \\\n(partition by ORIGIN order by cast(departure_time as int) range between 10800 preceding and 7200 preceding),0) as dep_delay_last_hour\\\n, ifnull(sum(ARR_DEL15) over (partition by DEST order by cast(departure_time as int) range between 10800 preceding and 7200 preceding)/ count(1) over \\\n                     (partition by DEST order by cast(departure_time as int) range between 10800 preceding and 7200 preceding),0) as arr_delay_last_hour \\\nfrom flight_adjusted order by FL_DATE, CRS_DEP_TIME\")\nflightDF.write.saveAsTable(\"flights_delay\")\n\n#44 seconds\nspark.sql(\"drop table if exists flights_add_features\")\nflight2DF = spark.sql(\"select f.* , ifnull(dep_delay_last_hour/ao.ratio,0) as dep_delay_ratio, arr_delay_last_hour/ad.arr_ratio as arr_delay_ratio \\\nfrom flights_delay f join airport_delay_analysis ao on f.ORIGIN = ao.ORIGIN \\\njoin airport_delay_analysis ad on f.DEST = ad.ORIGIN \")\nflight2DF.write.saveAsTable(\"flights_add_features\")\n\n#51 seconds\nspark.sql(\"drop table if exists flights_pagerank\")\ndbutils.fs.rm(\"dbfs:/user/hive/warehouse/flights_pagerank\", True)\nflight3DF = spark.sql(\"select f.* , ao.pagerank, ad.pagerank as pagerank_dest \\\nfrom flights_add_features f join airport_pagerank ao on f.ORIGIN = ao.id \\\njoin airport_pagerank ad on f.DEST = ad.id \")\nflight3DF.write.saveAsTable(\"flights_pagerank\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecd12a23-3406-44d2-a74a-cebfb6462e83"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# 2. Preparing weather data\nHere, we write a function that calculates the Haversine distance between airports, which is the closest distance between two points on a spheres. First, we calculate a used_stationsDF, which represents the weather stations. Then, for each airport, we calculate the distance to all weather stations, and keep track of the ones within a prespecified minimum distance. This way, we can find the closest weather station within a certain distance for each airport. This is registered as a separate table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8320e4a2-02a8-42cd-b6c8-d6b2e55679ec"}}},{"cell_type":"code","source":["used_stationsDF = spark.sql(\"select distinct concat(usaf, wban) as station, lat, lon from stations_tbl s where  begin < '20200101' and end >= '20150101' and lat is not null\").collect()\n\n# Calculate Haversine distance between airport and startions\ndef get_all_stations_within(latitude, longitude, distance):\n  def Haversine(lat1,lon1,lat2,lon2) :\n    '''Function for calculating Haversine distance between two coordinates'''\n    R = 6371.0088\n    lat1 = math.radians(float(lat1))\n    lon1 = math.radians(float(lon1))\n    lat2 = math.radians(float(lat2))\n    lon2 = math.radians(float(lon2))\n\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2) **2\n    c = 2 * np.arctan2(a**0.5, (1-a)**0.5)\n    d = R * c\n    return round(float(d),4)\n\n\n  airport_station_list = []\n  for s in used_stationsDF:\n    #print(latitude,longitude,s.lat,s.lon, end=\"\")\n    h = Haversine(latitude, longitude, s.lat, s.lon)\n    if h<distance:\n      airport_station_list.append([s.station, h])\n    #print(\" ---- \", h)\n    \n  return airport_station_list\nspark.udf.register(\"list_AllStationWithinUDF\", get_all_stations_within, \n                       ArrayType(StructType([StructField(\"station\", StringType()),\n                                  StructField(\"distance\", FloatType())])))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85512655-fbf5-4691-bef0-4938518ac0af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[7]: &lt;function __main__.get_all_stations_within(latitude, longitude, distance)&gt;</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[7]: &lt;function __main__.get_all_stations_within(latitude, longitude, distance)&gt;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.1 Find stations for each airport\n### Table: airport_station_maps\n\nHere, we create the weather stations table in order to find the best weather data point for each flight. In order to determine the best distance, we tried looking at distances around 10KM, 25KM or within 85KM. For most airports, we can find the necessary data by using a 25KM radius to search for stations. For BQN and PSE, we use 85KM radius as these are the closest stations available for these airports. This way, we were able to get weather data joined for all flights."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aee0c80f-ddb3-420c-8055-7dd165cfa710"}}},{"cell_type":"code","source":["%sql\n-- Create table mapping table between airport and station for a radius of 100KM\ndrop table if exists airport_station_maps;\ncreate table airport_station_maps (IATA STRING, station STRING, distance FLOAT);\n\nwith cte as (select a.IATA,explode(list_AllStationWithinUDF(a.latitude,a.longitude,100)) as stations\n  from airport_used_tbl au join airport_tbl a on a.IATA = au.ORIGIN \n  union all\n  select au.ORIGIN, explode(list_AllStationWithinUDF(a.latitude,a.longitude,100))\n  from airport_used_tbl au, airport_tbl a \n  where a.IATA = 'ISN' and au.ORIGIN = 'XWA' \n  union all \n  select au.ORIGIN, explode(list_AllStationWithinUDF(a.latitude,a.longitude,100))\n  from airport_used_tbl au, airport_tbl a \n  where a.ICAO = 'KTKI' and au.ORIGIN = 'TKI'\n)\ninsert into airport_station_maps\nselect IATA, stations.station, stations.distance\nfrom cte;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f7182c9-10a0-4f85-b75e-1808853026c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql('drop table if exists all_weather')\ndbutils.fs.rm('/delta/all_weather',True)\n\n# Pick weather data from closest station within 25KM, or 85KM (for BQN and PSE)\nspark.sql(\"select asm.IATA as ORIGIN, w.station, date, to_date(date) as date_key, elevation, WND, CIG, VIS, TMP, DEW, SLP \\\n      FROM weather_tbl w join airport_station_maps asm on w.station = asm.station \\\n      where (asm.distance < 25 or (asm.IATA in ('BQN', 'PSE') and asm.distance<85)) \\\n      and date between '2014-12-30' and '2020-01-01'\")\\\n    .write.partitionBy(\"ORIGIN\").format(\"delta\").save(\"/delta/all_weather\")\nspark.sql(\"CREATE TABLE all_weather USING DELTA LOCATION '/delta/all_weather'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22dc23e8-e527-467b-a2e7-ac24abb00d49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The weather data after joining with the stations table is very unclean. For example, some variables are labeled with a string of \"9\" when they are missing. We do not want to consider such rows when joining into the flights table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ad2366f-ffe2-47cf-930e-21382dc5edb5"}}},{"cell_type":"code","source":["%sql\n--5.35 minutes\n-- Merge flight time into weather table\ndelete from all_weather\nwhere WND='999,9,9,9999,9' and CIG ='99999,9,9,9' and VIS ='999999,9,9,9' and TMP='+9999,9' and DEW='+9999,9' and SLP='99999,9';\n\ninsert into all_weather \nselect distinct f.ORIGIN, null,  f.report_time_utc , null, null, null, null, null, null, null, null\nfrom flights_pagerank AS f left join all_weather AS w ON f.ORIGIN = w.ORIGIN AND f.report_time_utc=w.date\nwhere w.ORIGIN IS NULL\nUNION ALL\nselect distinct f.DEST, null,  f.report_time_utc , null, null, null, null, null, null, null, null\nfrom flights_pagerank AS f left join all_weather AS w ON f.DEST = w.ORIGIN AND f.report_time_utc=w.date\nwhere w.ORIGIN IS NULL;\n\n-- For PPG airport only - it's missing 2015 weather data\n-- We will use 2016 weather data insert back to 2015 and forward fill\ninsert into all_weather \nselect ORIGIN, null, date - INTERVAL 1 year, null, elevation, WND, CIG, VIS, tmp, DEW, SLP\nfrom all_weather\nwhere ORIGIN = 'PPG'\nand year(date)=2016;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f627e23f-3f00-41da-a73f-74c5ca866314"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.2 Parse weather data\n### Table: parsed_weather_data\n\nThe weather data is also encoded in a messy manner. For example, visibility has 3 fields representing distance, a classification of the distance as a code, and a quality of data code.  Here, we parse weather data into associated fields. Specific format for each field is extracted from the NOAA website:\n\nhttps://www.ncei.noaa.gov/data/global-hourly/doc/isd-format-document.pdf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1a53c41-2f43-4de5-bdf4-e41c48251055"}}},{"cell_type":"code","source":["# 16.75 minutes\nspark.sql(\"drop table if exists parsed_weather_data\")\ndbutils.fs.rm(\"/delta/parsed_weather_data\",True)\n\nspark.sql(\"Select ORIGIN, \\\n  date, \\\n  int(left(WND, 3)) as wnd_direction, \\\n  substring(WND, 5, 1) as wnd_dir_quality, \\\n  substring(WND, 7,1) as wnd_type, \\\n  int(substring(WND, 9,4))/10 as wnd_speed, \\\n  substring(WND, 14,1) as wnd_speed_quality, \\\n  int(left(CIG, 5)) as cig_height, \\\n  substring(CIG, 7,1) as cig_height_quality, \\\n  substring(CIG, 9,1) as cig_code, \\\n  right(CIG,1) as cig_cavok_code, \\\n  int(left(VIS,6)) as vis_distance, \\\n  substring(VIS, 8, 1) as vis_distance_quality, \\\n  substring(VIS, 10,1) as vis_var_code, \\\n  substring(VIS, 12,1) as vis_var_quality, \\\n  int(left(TMP, 5)) /10.0 as tmp, \\\n  substring(TMP,7,1) as tmp_quality, \\\n  int(left(DEW, 5)) /10.0 as dew, \\\n  substring(DEW, 7, 1) as dew_quality, \\\n  int(left(SLP, 5)) /10.0 as slp, \\\n  substring(SLP, 7, 1) as slp_quality, \\\n  elevation \\\nfrom all_weather\")\\\n.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"ORIGIN\").save(\"/delta/parsed_weather_data\")\n\nspark.sql(\"CREATE TABLE parsed_weather_data USING DELTA LOCATION '/delta/parsed_weather_data'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83275144-88d8-4295-a41a-f79b45298b5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[" %sql\n-- 18.99 minutes\n-- Clean up missing values\n-- wind\nupdate parsed_weather_data\nset wnd_direction = Null\nwhere wnd_dir_quality = 3 or wnd_dir_quality=7 or wnd_direction > 360;\n\n-- wind type code\n--update parsed_weather_data\n--set wnd_type = Null\n--where wnd_type = 9;\n\n-- WIND-OBSERVATION speed rate\n-- The rate of horizontal travel of air past a fixed point.\n-- MIN: 0000 MAX: 0900 UNITS: meters per second\nupdate parsed_weather_data\nset wnd_speed = Null\nwhere wnd_speed_quality =3 or wnd_speed_quality=7 or wnd_speed > 999;\n\n-- SKY-CONDTION-OBSERVATION ceiling quality code\n-- The code that denotes a quality status of a reported ceiling height dimension.\n-- DOM: A specific domain comprised of the characters in the ASCII character set\nupdate parsed_weather_data\nset cig_height = Null\nwhere cig_height_quality=3 or cig_height_quality=7 or cig_height > 22000;\n\n-- SKY-CONDITION-OBSERVATION ceiling determination code\n-- The code that denotes the method used to determine the ceiling.\n-- DOM: A specific domain comprised of the characters in the ASCII character set\n--update parsed_weather_data\n--set cig_code = Null\n--where cig_code = 9;\n\n-- SKY-CONDITION-OBSERVATION CAVOK code\n-- The code that represents whether the 'Ceiling and Visibility Okay' (CAVOK) condition has been reported.\n-- DOM: A specific domain comprised of the characters in the ASCII character set.\n--update parsed_weather_data\n--set cig_cavok_code = Null\n--where cig_cavok_code = 9;\n\n-- VISIBILITY-OBSERVATION distance dimension\n-- The horizontal distance at which an object can be seen and identified.\nupdate parsed_weather_data\nset vis_distance = Null\nwhere vis_distance_quality=3 or vis_distance_quality=7  or vis_distance > 160000;\n\n-- VISIBILITY-OBSERVATION variability code\n-- The code that denotes whether or not the reported visibility is variable.\n-- DOM: A specific domain comprised of the characters in the ASCII character set.\nupdate parsed_weather_data\nset vis_var_code = '9'\nwhere vis_var_quality=3 or vis_var_quality=7;\n\n-- temperature 999.9\nupdate parsed_weather_data\nset tmp = Null\nwhere TMP_quality='3' or TMP_quality='7' or tmp > 62;\n\n--AIR-TEMPERATURE-OBSERVATION dew point temperature\n-- The temperature to which a given parcel of air must be cooled at constant pressure and water vapor\n-- content in order for saturation to occur.\n-- MIN: -0982 MAX: +0368 UNITS: Degrees Celsius\nupdate parsed_weather_data\nset DEW = Null\nwhere DEW_quality='3' or DEW_quality='7' or dew>37;\n\n--ATMOSPHERIC-PRESSURE-OBSERVATION sea level pressure\n-- The air pressure relative to Mean Sea Level (MSL).\n-- MIN: 08600 MAX: 10900 UNITS: Hectopascals\nupdate parsed_weather_data\nset SLP = Null\nwhere SLP_quality='3' or SLP_quality='7' or slp>1100;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55fdc485-1a85-48a3-a3ec-a985a8daa3ba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2.3 Use Forward fill to fill the gaps in all_weather table\n\nIn order to get weather data for each flight, we have to have a weather data point available to merge. To get this, we first use forward fill in order to fill in gaps in the weather table. The idea is that if a weather data point is missing, the best available data point is the most recent available, which is achieved with a foward fill."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6967fb1b-9911-45ed-a29d-7bf007c5eaf8"}}},{"cell_type":"code","source":["# 8.34 minutes\n# Calculate weather per minute - take average when there are multiple records at same time\nspark.sql(\"drop table if exists weather_merged\")\ndbutils.fs.rm(\"/w261/weather_merged\", True)\n\nweather_merged = spark.sql(\"select distinct origin, date \\\n,(select avg(wnd_direction) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.wnd_dir_quality <> 3 and w2.wnd_dir_quality <> 7 and w2.wnd_direction is not null) as wnd_direction \\\n,(select min(wnd_type) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.wnd_type is not null)  as wnd_type \\\n,(select avg(wnd_speed) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.wnd_speed_quality <> 3 and w2.wnd_speed_quality <> 7 and w2.wnd_speed is not null ) as wnd_speed \\\n,(select avg(cig_height) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.cig_height_quality <> 3 and w2.cig_height_quality <> 7 and w2.cig_height is not null ) as cig_height \\\n,(select min(cig_code) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.cig_code<>'9' and w2.cig_code is not null) as cig_code \\\n,(select min(cig_cavok_code) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.cig_cavok_code<>'9' and w2.cig_cavok_code is not null) as cig_cavok_code \\\n,(select avg(vis_distance) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.vis_distance_quality<>3 and w2.vis_distance_quality<>7 and w2.vis_distance is not null) as vis_distance \\\n,(select min(vis_var_code) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.vis_var_quality<>3 and w2.vis_var_quality<>7 and w2.vis_var_code is not null) as vis_var_code \\\n,(select avg(tmp) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.TMP_quality<>'3' and w2.TMP_quality<>'7' and w2.tmp is not null) as tmp \\\n,(select avg(dew) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.dew_quality<>'3' and w2.DEW_quality<>'7' and w2.dew is not null) as dew \\\n,(select avg(slp) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date and w2.slp_quality<>'9' and w2.slp is not null) as slp \\\n,(select avg(elevation) from parsed_weather_data w2 where w2.ORIGIN=w.ORIGIN and w2.date = w.date) as elevation \\\nfrom parsed_weather_data w\")\n#weather_merged.write.mode(\"overwrite\").parquet(\"/w261/weather_merged\")\nweather_merged.write.saveAsTable(\"weather_merged\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"611986cc-077c-4ea3-a8ac-97ddc6f0b317"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 3.51 minutes\nimport sys\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import last, first\n\nspark.sql('drop table if exists weather_ffilled')\ndbutils.fs.rm('/delta/weather_ffilled',True)\n\nweather_df = spark.sql(\"select * from weather_merged\")\nwindow = Window.partitionBy('ORIGIN')\\\n               .orderBy('date')\\\n               .rowsBetween(-sys.maxsize,0)\n\n\n# define the forward-filled column\nwnd_dir_column = last(weather_df['wnd_direction'], ignorenulls=True).over(window)\n#wnd_dir_quality_column = last(weather_df['wnd_dir_quality'], ignorenulls=True).over(window)\nwnd_type_column = last(weather_df['wnd_type'], ignorenulls=True).over(window)\nwnd_speed_column = last(weather_df['wnd_speed'], ignorenulls=True).over(window)\n#wnd_speed_quality_column = last(weather_df['wnd_speed_quality'], ignorenulls=True).over(window)\ncig_height_column = last(weather_df['cig_height'], ignorenulls=True).over(window)\n#cig_height_quality_column = last(weather_df['cig_height_quality'], ignorenulls=True).over(window)\ncig_code_column = last(weather_df['cig_code'], ignorenulls=True).over(window)\ncig_cavok_code_column = last(weather_df['cig_cavok_code'], ignorenulls=True).over(window)\nvis_distance_column = last(weather_df['vis_distance'], ignorenulls=True).over(window)\n#vis_distance_quality_column = last(weather_df['vis_distance_quality'], ignorenulls=True).over(window)\nvis_var_column = last(weather_df['vis_var_code'], ignorenulls=True).over(window)\n#vis_var_quality_column = last(weather_df['vis_var_quality'], ignorenulls=True).over(window)\ntmp_column = last(weather_df['tmp'], ignorenulls=True).over(window)\n#tmp_quality_column = last(weather_df['tmp_quality'], ignorenulls=True).over(window)\ndew_column = last(weather_df['DEW'], ignorenulls=True).over(window)\n#dew_quality_column = last(weather_df['dew_quality'], ignorenulls=True).over(window)\nslp_column = last(weather_df['SLP'], ignorenulls=True).over(window)\n#slp_quality_column = last(weather_df['slp_quality'], ignorenulls=True).over(window)\nele_column = last(weather_df['elevation'], ignorenulls=True).over(window)\n\n# do the fill\n\nweather_df_filled = weather_df\\\n  .withColumn('wnd_direction', wnd_dir_column)\\\n  .withColumn('wnd_type', wnd_type_column)\\\n  .withColumn('wnd_speed', wnd_speed_column)\\\n  .withColumn('cig_height', cig_height_column)\\\n  .withColumn('cig_code', cig_code_column)\\\n  .withColumn('cig_cavok_code', cig_cavok_code_column)\\\n  .withColumn('vis_distance', vis_distance_column)\\\n  .withColumn('vis_var_code', vis_var_column)\\\n  .withColumn('tmp', tmp_column)\\\n  .withColumn('dew', dew_column)\\\n  .withColumn('SLP', slp_column)\\\n  .withColumn('elevation', ele_column).cache()\n\nweather_df_filled.select('ORIGIN', 'date', 'wnd_direction', 'wnd_type','wnd_speed', 'cig_height','cig_code','cig_cavok_code','vis_distance','vis_var_code', 'tmp', 'DEW', 'SLP', 'elevation')\\\n  .dropDuplicates()\\\n  .write.format(\"delta\").mode(\"overwrite\").partitionBy(\"ORIGIN\").save(\"/delta/weather_ffilled/\")\n\n#weather_df_filled.write.saveAsTable(\"weather_ffilled\")\n\nspark.sql(\"CREATE TABLE weather_ffilled USING DELTA LOCATION '/delta/weather_ffilled'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7528f6cd-6e39-491b-9a36-e2be88fe0436"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n--Handle still missing values\n--For catogical \nupdate weather_ffilled\nset wnd_type = 9\nwhere wnd_type is null;\n\nupdate weather_ffilled\nset cig_code = 9\nwhere cig_code is null;\n\nupdate weather_ffilled\nset cig_cavok_code = 9 \nwhere cig_cavok_code is null;\n\nupdate weather_ffilled\nset vis_var_code=9\nwhere vis_var_code is null;\n\n--missing one tmp for GUM at 2015-01-01\n--update weather_ffilled\n--set wnd_direction = 60, wnd_speed = 6.7, tmp=29, dew=23,slp=1010, elevation = 77.4, cig_height=22000\n--where origin='GUM' and date < '2015-01-01'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e09cd4ab-fbea-49bf-bfc6-e2d108abfa0f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql('drop table if exists weather_cleaned')\ndbutils.fs.rm('/w261/weather_cleaned',True)\ndbutils.fs.rm('/user/hive/warehouse/weather_cleaned', True)\n\nweatherDF = spark.sql(\"select ORIGIN, date, \\\nifnull(wnd_direction, avg(wnd_direction) over (partition by ORIGIN order by w.date asc rows between 96 preceding AND 96 following)) as wnd_direction, \\\nwnd_type, \\\nifnull(wnd_speed, avg(wnd_speed) over (partition by ORIGIN order by w.date asc rows between 5 preceding AND 5 following)) as wnd_speed, \\\nifnull(cig_height, avg(cig_height) over (partition by ORIGIN order by w.date asc rows between 12 preceding AND 12 following)) as cig_height, \\\ncig_code, \\\ncig_cavok_code, \\\nifnull(vis_distance, avg(vis_distance) over (partition by ORIGIN order by w.date asc rows between 48 preceding AND 48 following)) as vis_distance, \\\nvis_var_code, \\\nifnull(tmp, avg(tmp) over (partition by ORIGIN order by w.date asc rows between 5 preceding AND 5 following)) as tmp, \\\nifnull(dew, avg(dew) over (partition by ORIGIN order by w.date asc rows between 12 preceding AND 12 following)) as dew, \\\nifnull(dew, avg(elevation) over (partition by ORIGIN order by w.date asc rows between 12 preceding AND 12 following)) as elevation \\\nfrom weather_ffilled w\")\n\nweatherDF.write.mode(\"overwrite\").parquet(\"/w261/weather_cleaned\")\nweatherDF.write.saveAsTable(\"weather_cleaned\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43a86c70-a53a-4d14-b5b6-728a74f7e328"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3. Final Step: Merge Data into one dataset\n### Table: flights_all\n\n\nThe final step for generating the flights_all table is to fill any remaining missing values (such as at the beginning of the dataset) using average neighboring values (we also tried Backward Fill, but this attempt failed). We also merge the weather dataset and flight dataset. The principal of the final dataset is to create one united dateset with as many original features as possible, which allows others to create more features during model exploring and training step."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06201dde-6e53-49fd-9628-d028a98a9bea"}}},{"cell_type":"code","source":["# 2.52\nspark.sql(\"drop table if exists flights_all\")\ndbutils.fs.rm(\"/w261/flights_all\", True)\n\n# Merge into one dataset\n# 6 minutes\n\nall_dataDF = spark.sql(\"select f.* \\\n                    , w.wnd_direction, w.wnd_speed, w.cig_height, w.cig_code, w.cig_cavok_code, w.vis_distance, w.vis_var_code, w.tmp, w.dew, w.elevation \\\n                    , wd.wnd_direction as dest_wnd_direction, wd.wnd_speed as dest_wnd_speed, wd.cig_height as dest_cig_height, wd.cig_code as dest_cig_code \\\n                    , wd.cig_cavok_code as dest_cig_cavok_code, wd.vis_distance as dest_vis_distance, wd.vis_var_code as dest_vis_var_code \\\n                    , wd.tmp as dest_tmp, wd.dew as dest_dew, wd.elevation as dest_elevation \\\n                    from flights_pagerank f join weather_cleaned w on f.ORIGIN=w.ORIGIN and f.report_time_utc = w.date \\\n                    join weather_cleaned wd on f.DEST=wd.ORIGIN and f.report_time_utc = wd.date\")\n\n#all_dataDF.dropDuplicates().write.mode(\"overwrite\").parquet(\"/w261/flights_all\")\nall_dataDF = all_dataDF.withColumn(\"mono_index\", f.monotonically_increasing_id())\nall_dataDF.write.saveAsTable(\"flights_all\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6cb1c1b-e3c6-4fca-9222-d9e43fcd0052"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Flights_all_v2\nUpdate: 2020-11-30 : Adding TAIL_NUM group\n\nWe know that different TAIL_NUMs, which represent different flights and their associated paths, have different average values. As a result, it is reasonable to have a feature that classifies flight TAIL_NUM based on their group."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a6453da-63e2-4c2c-9b84-1ddb6194404d"}}},{"cell_type":"code","source":["spark.sql(\"drop table if exists tail_num_group\")\ntail_group = spark.sql(\"with delayCTE as (select tail_num, count(*) as total_flights, avg(ifnull(dep_delay,0)) as avg_dep_delay from flights_all \\\nwhere tail_num is not null group by tail_num) \\\nselect tail_num, total_flights, avg_dep_delay, case  when avg_dep_delay < 15 then 1 when avg_dep_delay < 30 then 2 when avg_dep_delay < 45 then 3 else 4 end as tail_delay_group \\\nfrom delayCTE \\\norder by avg_dep_delay\").write.saveAsTable(\"tail_num_group\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ae7d23c-1eaf-414f-95d0-72cbe2b5d0ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"drop table if exists flights_all_v2\")\nspark.sql(\"select f.*, ifnull(t.tail_delay_group, 0) as tail_delay_group from flights_all f left join tail_num_group t on f.TAIL_NUM = t.TAIL_NUM\").write.saveAsTable(\"flights_all_v2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"033ab8ed-55bd-46c1-a4a8-a238751da5ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Flights_all_v3\n\nAdding DEP_DEL15_PREV (one of our most important variables)\n\nWe believe an important feature is whether or not the previous flight of a plane was delayed in terms of its departure. We only consider this indicator variable as 1 if the plane's previous delay IF the previous flight on the same plane left at least 2 hours before current departure time (since we want to predict 2 hours ahead of time, for shorter flights, this information will not exist). We get this feature by performing a self-join of the flights_all table based on the TAIL_NUM. In order to get the most recent flight at least 2hrs before with the same TAIL_NUM, we generate a row number partitioned by the current flight considered, and ordered by the departure time of the previous flight with the same TAIL_NUM, and then take the first row. We also restrict origin of the previous flight to be the destination of current flight, which optimizes this query and prevents a full join on the TAIL_NUM."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d414344-0c88-4ef1-9897-6800e654b7e2"}}},{"cell_type":"code","source":["spark.sql(\"drop table if exists flights_all_3\")\ndbutils.fs.rm(\"/user/hive/warehouse/flights_all_3\", True)\n\n\nflights_v3 = spark.sql('''SELECT e1.*, d1.DEP_DEL15_PREV FROM(SELECT * FROM (SELECT e1.mono_index, e1.departure_time , e1.TAIL_NUM, e1.TAIL_NUM TAIL_NUM_PREV, e1.ORIGIN_CITY_NAME, e1.DEST_CITY_NAME, e2.DEST_CITY_NAME DEST_CITY_NAME_PREV, e2.DEP_DEL15 AS DEP_DEL15_PREV, e2.departure_time AS departure_time_prev, ROW_NUMBER() OVER (PARTITION BY e1.mono_index ORDER BY e2.departure_time DESC) AS rn \nFROM flights_all e1, flights_all e2\nWHERE e1.TAIL_NUM = e2.TAIL_NUM \nAND e1.departure_time > e2.departure_time + INTERVAL 2 HOURS\nAND e1.ORIGIN_AIRPORT_ID = e2.DEST_AIRPORT_ID) WHERE rn = 1) d1\nLEFT JOIN flights_all e1\nON d1.mono_index = e1.mono_index\nORDER BY departure_time\n''')\n\nflights_v3.na.fill(0).write.saveAsTable('flights_all_3')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6501762-d2f5-4ace-8ca1-7cbc87f68f11"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Flights_all_v4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b926302-7f41-4c72-b64e-2494534c2a1d"}}},{"cell_type":"markdown","source":["We generate a variable OD_group based on the average delay grouped by both ORIGIN and DEST (flights of a particular path). Here, 0 represents less than 5 flights are delayed in 5 years, 1 means the average delay (per flight) is less than 5 minutes, 2 means the average delay is less 10 minutes, and 3 means the average delay is less 15 minutes, and 4 otherwise."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50553057-2d92-40d7-893d-10496df8d522"}}},{"cell_type":"code","source":["spark.sql(\"drop table if exists OD_group\")\n\ntail_group = spark.sql(\"with delayCTE as (select ORIGIN, DEST, count(*) as total_flights, avg(ifnull(dep_delay,0)) as avg_dep_delay from flights_all_3 \\\ngroup by ORIGIN, DEST having total_flights >= 5) \\\nselect ORIGIN, DEST, total_flights, avg_dep_delay, int(case  when avg_dep_delay < 5 then 1 when avg_dep_delay < 10 then 2 when avg_dep_delay < 15 then 3 else 4 end) as OD_GROUP \\\nfrom delayCTE \\\norder by avg_dep_delay\").write.saveAsTable(\"OD_group\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53b09c4c-0973-43da-94dc-28eae790e527"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"drop table if exists flights_all_v4\")\nspark.sql(\"select f.*, \\\n          ifnull(o.OD_GROUP, 0) as OD_GROUP \\\n          from flights_all_3 f left join tail_num_group t on f.TAIL_NUM = t.TAIL_NUM \\\n          left join OD_group o on f.ORIGIN=o.ORIGIN and f.DEST = o.DEST\").write.saveAsTable(\"flights_all_v4\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22ec98e9-4511-413d-8430-1fd9ce6f34fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Final flight table with all features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28bb56f4-6d10-4fff-90da-2a81f0ad63ab"}}},{"cell_type":"markdown","source":["For the remaining features, we first split the table in train and test sets (90/10 split). This is because these variables will be calculated based on aggregate statistics, and we do not want test data information to leak into the training."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2c2e5e4-c6d5-45da-a230-1ec6508faacc"}}},{"cell_type":"code","source":["%sql\nrefresh table flights_all_v4"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c56f467-540f-4412-90a7-98a3ff4a7e85"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Generate view of the data table we have so far\ndata = spark.sql(\"SELECT * FROM flights_all_v4 WHERE DEP_DELAY IS NOT NULL and OP_CARRIER NOT IN ('US', 'VX') and ORIGIN NOT IN ('CEC', 'CLD', 'ILG', 'DIK', 'EFD', 'ENV', 'TKI', 'UST', 'FNL', 'YNG', 'IFP', 'FLO') and DEST NOT IN ('CEC', 'CLD', 'ILG', 'DIK', 'EFD', 'ENV', 'TKI', 'UST', 'FNL', 'YNG', 'IFP', 'FLO') ORDER BY FL_DATE, departure_time\")\ndata.registerTempTable('flights_all')\ndf_total = data.withColumn(\"mono_index\", f.monotonically_increasing_id())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e65dee8a-64a6-4561-bf42-97bb2f7ae6e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#train test split based on time. First 90% is train\ncount = df_total.count()\ntrain = df_total.limit(int(0.9*count))\ntest = df_total.orderBy(f.desc(\"mono_index\")).drop(\"mono_index\").limit(count-int(0.9*count))\ntrain.registerTempTable('flights_train')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9e9876e-cc93-4b73-9f15-7ae78efdc53f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The inspiration behind further feature engineering is that there are several variables with a very large number of categories. This is demonstrated below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc5c1511-5f66-4a11-bec6-e5d80a8e8118"}}},{"cell_type":"code","source":["#List of variables we have\n\nint_categorical = ['DAY_OF_WEEK', 'DEP_DEL15_PREV', 'MONTH', 'QUARTER', 'DAY_OF_MONTH', 'OP_CARRIER_AIRLINE_ID','OP_CARRIER_FL_NUM', 'CRS_DEP_TIME','DISTANCE_GROUP']#, 'MONTH', 'QUARTER', 'DAY_OF_MONTH']#[\"OP_CARRIER_AIRLINE_ID\", 'ORIGIN_AIRPORT_ID', 'DEST_AIRPORT_ID']\n\nstr_features = ['DEST_STATE_ABR', 'wnd_direction', 'dest_wnd_direction','OP_UNIQUE_CARRIER','OP_CARRIER', 'ORIGIN', 'ORIGIN_STATE_ABR', 'DEST', 'cig_code','cig_cavok_code','dest_cig_code','dest_cig_cavok_code','dest_vis_var_code'] #eliminated tail num, 'ORIGIN', 'DEST', 'vis_var_code', 'dest_vis_var_code'\n\ncategorical_features = int_categorical + str_features\nnum_features = [\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height','wnd_direction'] \n\nfe = []\nfor v in categorical_features:\n  fe.append((v, df_total.select(v).distinct().count()))\nfe.sort(key = lambda x: -x[1])\nfe = [i for i in fe if i[1] > 31]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b83ccb7-4230-4396-90dd-9033b6f614dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#categorical variables with more 31 categories (allowing the number of days to be a separate variables)\nfe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fcfaa099-ead2-4495-86a0-52d68b348ee1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[26]: [(&#39;OP_CARRIER_FL_NUM&#39;, 7176),\n (&#39;dest_wnd_direction&#39;, 2193),\n (&#39;wnd_direction&#39;, 2168),\n (&#39;CRS_DEP_TIME&#39;, 1433),\n (&#39;ORIGIN&#39;, 360),\n (&#39;DEST&#39;, 360),\n (&#39;DEST_STATE_ABR&#39;, 52),\n (&#39;ORIGIN_STATE_ABR&#39;, 52)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[26]: [(&#39;OP_CARRIER_FL_NUM&#39;, 7176),\n (&#39;dest_wnd_direction&#39;, 2193),\n (&#39;wnd_direction&#39;, 2168),\n (&#39;CRS_DEP_TIME&#39;, 1433),\n (&#39;ORIGIN&#39;, 360),\n (&#39;DEST&#39;, 360),\n (&#39;DEST_STATE_ABR&#39;, 52),\n (&#39;ORIGIN_STATE_ABR&#39;, 52)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We see that for example, for the carrier group, there are 7175 distinct flight numbers. Even though the flight number likely plays an important role in determining delays, it is unlikely our model can capture the effect of each number. Instead, one way to bin different carriers into the same bin is to determine the average flight delay of the flight number based on the training set. For each variable with more than 31 categories (so that we don't need to bin days of the month), we first generate a percent rank for the mean response based on each group of the variable. For example, with OP_CARRIER_FL_NUM, for each distinct number, we calculate the mean fraction of flights delayed. We then calculate a percent rank for this mean response, and we then bin this percent rank into 10 groups (10 indicators for our models to use). This way, we greatly reduce the number of indicators we need, with each indicator representing a different mean level of delays, giving our model a much better chance at making use of these 10 groups (rather than a giant number of groups, which could lead to overfitting and too much flexibility)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8311f3d-c9d2-4923-a382-dbece7127a9e"}}},{"cell_type":"code","source":["pct_table_dep_time = spark.sql('''SELECT CRS_DEP_TIME,\nCASE\n    WHEN pct_rank < 0.10 THEN 0\n    WHEN pct_rank < 0.20 THEN 1\n    WHEN pct_rank < 0.30 THEN 2\n    WHEN pct_rank < 0.40 THEN 3\n    WHEN pct_rank < 0.50 THEN 4\n    WHEN pct_rank < 0.60 THEN 5\n    WHEN pct_rank < 0.70 THEN 6\n    WHEN pct_rank < 0.80 THEN 7\n    WHEN pct_rank < 0.90 THEN 8\n    ELSE 9\nEND AS CRS_DEP_TIME_bucket\nFROM \n(SELECT CRS_DEP_TIME, PERCENT_RANK() OVER (ORDER BY avg_del) AS pct_rank FROM\n(SELECT CRS_DEP_TIME, AVG(DEP_DEL15) avg_del FROM flights_train\nGROUP BY CRS_DEP_TIME) t1)''')\n\npct_table_carrier = spark.sql('''SELECT OP_CARRIER_FL_NUM,\nCASE\n    WHEN pct_rank < 0.10 THEN 0\n    WHEN pct_rank < 0.20 THEN 1\n    WHEN pct_rank < 0.30 THEN 2\n    WHEN pct_rank < 0.40 THEN 3\n    WHEN pct_rank < 0.50 THEN 4\n    WHEN pct_rank < 0.60 THEN 5\n    WHEN pct_rank < 0.70 THEN 6\n    WHEN pct_rank < 0.80 THEN 7\n    WHEN pct_rank < 0.90 THEN 8\n    ELSE 9\nEND AS OP_CARRIER_FL_NUM_bucket\nFROM \n(SELECT OP_CARRIER_FL_NUM, PERCENT_RANK() OVER (ORDER BY avg_del) AS pct_rank FROM\n(SELECT OP_CARRIER_FL_NUM, AVG(DEP_DEL15) avg_del FROM flights_train\nGROUP BY OP_CARRIER_FL_NUM) t1)''')\n\npct_table_origin = spark.sql('''SELECT ORIGIN,\nCASE\n    WHEN pct_rank < 0.10 THEN 0\n    WHEN pct_rank < 0.20 THEN 1\n    WHEN pct_rank < 0.30 THEN 2\n    WHEN pct_rank < 0.40 THEN 3\n    WHEN pct_rank < 0.50 THEN 4\n    WHEN pct_rank < 0.60 THEN 5\n    WHEN pct_rank < 0.70 THEN 6\n    WHEN pct_rank < 0.80 THEN 7\n    WHEN pct_rank < 0.90 THEN 8\n    ELSE 9\nEND AS ORIGIN_bucket\nFROM \n(SELECT ORIGIN, PERCENT_RANK() OVER (ORDER BY avg_del) AS pct_rank FROM\n(SELECT ORIGIN, AVG(DEP_DEL15) avg_del FROM flights_train\nGROUP BY ORIGIN) t1)''')\n\npct_table_dest = spark.sql('''SELECT DEST,\nCASE\n    WHEN pct_rank < 0.10 THEN 0\n    WHEN pct_rank < 0.20 THEN 1\n    WHEN pct_rank < 0.30 THEN 2\n    WHEN pct_rank < 0.40 THEN 3\n    WHEN pct_rank < 0.50 THEN 4\n    WHEN pct_rank < 0.60 THEN 5\n    WHEN pct_rank < 0.70 THEN 6\n    WHEN pct_rank < 0.80 THEN 7\n    WHEN pct_rank < 0.90 THEN 8\n    ELSE 9\nEND AS DEST_bucket\nFROM \n(SELECT DEST, PERCENT_RANK() OVER (ORDER BY avg_del) AS pct_rank FROM\n(SELECT DEST, AVG(DEP_DEL15) avg_del FROM flights_train\nGROUP BY DEST) t1)''')\n\npct_table_origin_abr = spark.sql('''SELECT ORIGIN_STATE_ABR,\nCASE\n    WHEN pct_rank < 0.10 THEN 0\n    WHEN pct_rank < 0.20 THEN 1\n    WHEN pct_rank < 0.30 THEN 2\n    WHEN pct_rank < 0.40 THEN 3\n    WHEN pct_rank < 0.50 THEN 4\n    WHEN pct_rank < 0.60 THEN 5\n    WHEN pct_rank < 0.70 THEN 6\n    WHEN pct_rank < 0.80 THEN 7\n    WHEN pct_rank < 0.90 THEN 8\n    ELSE 9\nEND AS ORIGIN_STATE_ABR_bucket\nFROM \n(SELECT ORIGIN_STATE_ABR, PERCENT_RANK() OVER (ORDER BY avg_del) AS pct_rank FROM\n(SELECT ORIGIN_STATE_ABR, AVG(DEP_DEL15) avg_del FROM flights_train\nGROUP BY ORIGIN_STATE_ABR) t1)''')\n\npct_table_dest_abr = spark.sql('''SELECT DEST_STATE_ABR,\nCASE\n    WHEN pct_rank < 0.10 THEN 0\n    WHEN pct_rank < 0.20 THEN 1\n    WHEN pct_rank < 0.30 THEN 2\n    WHEN pct_rank < 0.40 THEN 3\n    WHEN pct_rank < 0.50 THEN 4\n    WHEN pct_rank < 0.60 THEN 5\n    WHEN pct_rank < 0.70 THEN 6\n    WHEN pct_rank < 0.80 THEN 7\n    WHEN pct_rank < 0.90 THEN 8\n    ELSE 9\nEND AS DEST_STATE_ABR_bucket\nFROM \n(SELECT DEST_STATE_ABR, PERCENT_RANK() OVER (ORDER BY avg_del) AS pct_rank FROM\n(SELECT DEST_STATE_ABR, AVG(DEP_DEL15) avg_del FROM flights_train\nGROUP BY DEST_STATE_ABR) t1)''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8093caa0-db92-4dde-a734-b4169dafbfbd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Join to our flights table (join to the entire table, even though the bins are calculated only using the training set)\ndf_full = df_total.join(pct_table_dep_time, 'CRS_DEP_TIME')\\\n        .join(pct_table_carrier, 'OP_CARRIER_FL_NUM')\\\n        .join(pct_table_origin, 'ORIGIN')\\\n        .join(pct_table_dest, 'DEST')\\\n        .join(pct_table_origin_abr, 'ORIGIN_STATE_ABR')\\\n        .join(pct_table_dest_abr, 'DEST_STATE_ABR')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"801869ce-cc7e-4d5f-bf50-441d135cdd31"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"drop table if exists flights_all_v5_2\")\ndf_full.orderBy('FL_DATE', 'CRS_DEP_TIME').drop('mono_index').write.saveAsTable(\"flights_all_v5_2\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"685732e4-d6ea-4a40-994a-a5f53baf2d44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###Summary of all variables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97828646-739b-437e-89ac-6d0e39d7779c"}}},{"cell_type":"markdown","source":["Below is a summary of all of the variables we consider in our models. Description for engineered variables are given."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d77d6cf1-8b68-4c8c-a912-414bbed1c3d2"}}},{"cell_type":"markdown","source":["| Variable      | Description (if needed)|\n| ----------- | ----------- |\n| DAY_OF_WEEK      |       |\n| DEP_DEL15_PREV   | Whether previous flight on the same plane was delayed|\n| MONTH      |       |\n| QUARTER   | |\n| OD_GROUP   | Average delay group for particular flight path |\n| OP_CARRIER_FL_NUM_bucket      | bucket approach (previous section) for flight number|\n| CRS_DEP_TIME_bucket   | bucket approach (previous section) for departure time|\n| ORIGIN_bucket      | bucket approach (previous section) for origin      |\n| DEST_bucket   | bucket approach (previous section) for destination|\n| ORIGIN_STATE_ABR_bucket      | bucket approach (previous section) for origin state      |\n| DEST_STATE_ABR_bucket   | bucket approach (previous section) for destination state|\n| DISTANCE      | distance to destination       |\n| vis_distance   | first field of visibility representing distance|\n| dest_vis_distance      |  |\n| tmp      | temperature of origin    |\n| dest_tmp   ||\n| dew   | dew point of origin|\n| dest_dew      |   |\n| elevation      | elevation of origin |\n| dest_elevation   | |\n| wnd_speed      | wind speed of origin   |\n| dest_wnd_speed   | |\n| pagerank      | pagerank score of origin airport |\n| pagerank_dest   | |\n| cig_height   | cloud height ceiling of origin|\n| dest_cig_height      |   |\n| cig_code      | cloud height ceiling of origin classification by NOAA      |\n| dest_cig_code      |   |\n| cig_cavok_code   | |\n| dest_cig_cavok_code   | |\n| vis_var_code      | visibility of origin classification by NOAA   |\n| dest_vis_var_code   | |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0fa37baa-0bb2-4a7b-8865-b496f4d27cd5"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cec890a-92e9-405d-980c-1122ba188174"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1. Preprocessing and Feature Engineering","dashboards":[],"language":"python","widgets":{},"notebookOrigID":598328949241557}},"nbformat":4,"nbformat_minor":0}
