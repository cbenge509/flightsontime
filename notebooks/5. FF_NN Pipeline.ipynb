{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType, FloatType\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.sql.functions import hour, minute, second, to_timestamp, monotonically_increasing_id, row_number, lit, pow, percent_rank\nfrom pyspark.sql.window import Window\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n\n#from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\nfrom pyspark.ml import Pipeline\n\nfrom petastorm.spark.spark_dataset_converter import _convert_vector\nfrom petastorm.pytorch import DataLoader, BatchedDataLoader\nfrom petastorm import make_batch_reader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer, required\n#from torch.utils.data.distributed import DistributedSampler\n\n\n# for distributed computing\nimport horovod.torch as hvd\nfrom sparkdl import HorovodRunner\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n\n# set number of cores; db says use less\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"120\")\n\n# enable Arrow support.\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\nspark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"200\")\nspark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\n\n# set device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7a4fc3d-9034-4790-b2fa-7a4884051c23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\n# number of workers\nsc._jsc.sc().getExecutorMemoryStatus().size()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc58901b-891e-4349-b6e4-33163d559655"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[84]: 1</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[84]: 1</div>"]}}],"execution_count":0},{"cell_type":"code","source":["PYTORCH_DIR = '/dbfs/ml/horovod_pytorch/take2'\n \nLOG_DIR = os.path.join(PYTORCH_DIR, 'PetaFlights')\nif os.path.exists(LOG_DIR) == False:\n    os.makedirs(LOG_DIR)\n    \ndef save_checkpoint(model, optimizer, epoch):\n  filepath = LOG_DIR + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n  state = {\n    'model': model.state_dict(),\n    'optimizer': optimizer.state_dict(),\n  }\n  torch.save(state, filepath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Setup Checkpoints","showTitle":true,"inputWidgets":{},"nuid":"5c32e3be-8fb0-44e4-b1b2-3b518286157e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# load from view\ndf = spark.sql(\"select * from flights_all_v5\")\n\n# filter\ndf = df.filter(f.col('DEP_DELAY') >= -15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3369e878-7eb5-45a7-805f-dc2248710593"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# create unique tail\ndf = df.withColumn('unique_tail', f.concat(f.col(\"TAIL_NUM\"), lit(\" \"), f.col(\"OP_UNIQUE_CARRIER\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fa33d84-1383-48dd-bf5b-29b224b3e02e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# create time conditions\nw1 = Window.partitionBy().orderBy('unique_tail', 'departure_time')\nw2 = Window.partitionBy('case_id').orderBy('unique_tail', 'departure_time')\n\ndf = df.withColumn(\"case_id\", f.sum(f.when(~(f.col(\"unique_tail\") == f.lag(\"unique_tail\").over(w1)) | (f.lag(\"DEP_DEL15\",1,0).over(w1) == 1),1).otherwise(0)).over(w1)+1) \\\n    .withColumn('time', f.count('*').over(w2)-1)\n\n# create time polynomial\ndf = df.withColumn('time2', f.pow(f.col(\"time\"), 2))\ndf = df.withColumn('time3', f.pow(f.col(\"time\"), 3))\n\n# sort by date\ndf = df.orderBy('unique_tail', 'departure_time', ascending=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"64f2872e-545d-42c7-88de-525ad7a79e3d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# target, features\ndf = df.select('DEP_DEL15', \"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height', 'departure_time', 'DEST_STATE_ABR', 'wnd_direction', 'dest_wnd_direction', 'OP_UNIQUE_CARRIER','OP_CARRIER', 'ORIGIN', 'ORIGIN_STATE_ABR', 'DEST', 'cig_code','cig_cavok_code','dest_cig_code','dest_cig_cavok_code','dest_vis_var_code', 'unique_tail', 'DAY_OF_WEEK', 'DEP_DEL15_PREV', 'MONTH', 'QUARTER', 'DAY_OF_MONTH', 'OP_CARRIER_AIRLINE_ID','OP_CARRIER_FL_NUM', 'DISTANCE_GROUP', 'OD_GROUP')\n\n# drop some problematic NAs\ndf = df.na.drop(subset=[\"DEP_DEL15\", 'unique_tail'])\n\n# limit for small batch testing\n#df = df.limit(130459)\n\n# create a variable that will be used to split the data into train/valid/test later\ndf = df.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"departure_time\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54fc9055-df3d-48bf-935f-bacf6141d952"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# list of str features\nstr_features = ['DEST_STATE_ABR', 'wnd_direction', 'dest_wnd_direction','OP_UNIQUE_CARRIER','OP_CARRIER', 'ORIGIN', 'ORIGIN_STATE_ABR', 'DEST', 'cig_code','cig_cavok_code','dest_cig_code','dest_cig_cavok_code','dest_vis_var_code', 'unique_tail']\n\n# list of int categorical features\nint_categorical = ['DAY_OF_WEEK', 'DEP_DEL15_PREV', 'MONTH', 'QUARTER', 'DAY_OF_MONTH', 'OP_CARRIER_AIRLINE_ID','OP_CARRIER_FL_NUM', 'DISTANCE_GROUP', 'OD_GROUP']\n\n# create indexers\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_cat\").fit(df) for column in str_features+int_categorical]\n\n# pipeline them\npipeline = Pipeline(stages=indexers)\n\n# transform -- drop str and int features listed above\ndf = pipeline.fit(df).transform(df).drop('DEST_STATE_ABR', 'wnd_direction', 'dest_wnd_direction','OP_UNIQUE_CARRIER','OP_CARRIER', 'ORIGIN', 'ORIGIN_STATE_ABR', 'DEST', 'cig_code','cig_cavok_code','dest_cig_code','dest_cig_cavok_code','dest_vis_var_code', 'unique_tail', 'DAY_OF_WEEK', 'DEP_DEL15_PREV', 'MONTH', 'QUARTER', 'DAY_OF_MONTH', 'OP_CARRIER_AIRLINE_ID','OP_CARRIER_FL_NUM', 'DISTANCE_GROUP')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Convert String and Int. Categorical to Zero-min Categorical","showTitle":true,"inputWidgets":{},"nuid":"60c80b70-093a-49f0-ab4d-5c1e2cce7a4b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["cont_features = [\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Continuous Features","showTitle":true,"inputWidgets":{},"nuid":"5d2cc639-eb5c-4836-ba4e-84b6efded80d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["cat_features = ['DEST_STATE_ABR_cat', 'wnd_direction_cat', 'dest_wnd_direction_cat','OP_UNIQUE_CARRIER_cat','OP_CARRIER_cat', 'ORIGIN_cat', 'ORIGIN_STATE_ABR_cat', 'DEST_cat', 'cig_code_cat','cig_cavok_code_cat','dest_cig_code_cat','dest_cig_cavok_code_cat','dest_vis_var_code_cat', 'unique_tail_cat', 'DAY_OF_WEEK_cat', 'DEP_DEL15_PREV_cat', 'MONTH_cat', 'QUARTER_cat', 'DAY_OF_MONTH_cat', 'OP_CARRIER_AIRLINE_ID_cat','OP_CARRIER_FL_NUM_cat', 'DISTANCE_GROUP_cat']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Categorical Features","showTitle":true,"inputWidgets":{},"nuid":"fe7fca91-7be1-4b58-b2a0-baa0740d9510"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["train_df = df.where(\"rank <= .8\").drop(\"rank\", \"departure_time\")\nval_df = df.where(\"rank > .8 and rank < .9\").drop(\"rank\", \"departure_time\")\ntest_df = df.where(\"rank >= .9\").drop(\"rank\", \"departure_time\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create Time Series Train / Valid Sets","showTitle":true,"inputWidgets":{},"nuid":"63ff4f44-b6d7-45b6-b48f-240eee003eb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# select features\nassembler = VectorAssembler(inputCols=cont_features, outputCol=\"features\")\n\n# create vector train_df\nassembled_train = assembler.transform(train_df).drop(\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height')\n\n# create vector val_df\nassembled_val = assembler.transform(val_df).drop(\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height')\n\n# create vector test_df\nassembled_test = assembler.transform(test_df).drop(\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Assemble Continuous Featuers","showTitle":true,"inputWidgets":{},"nuid":"17936a2a-a1f5-42a4-a1e6-22b335bbb0c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# scale train\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True).fit(assembled_train)\nassembled_train = scaler.transform(assembled_train).drop('features')\nassembled_train = _convert_vector(assembled_train, 'float32')\n\n# scale val \nassembled_val = scaler.transform(assembled_val).drop('features')\nassembled_val = _convert_vector(assembled_val, 'float32')\n\n# scale test \nassembled_test = scaler.transform(assembled_test).drop('features')\nassembled_test = _convert_vector(assembled_test, 'float32')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Scale Continuous Features","showTitle":true,"inputWidgets":{},"nuid":"17ce693a-21a4-497b-b3e1-eb6812176402"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# check partition size\nassembled_val.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4059ed71-c18c-453a-ae39-605a04216050"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# write train to parquet; at least to # of workers\n# write to dbfs/ml for extra speed performance\nassembled_train.repartition(10) \\\n  .write.mode(\"overwrite\") \\\n  .option(\"parquet.block.size\", 1024 * 1024) \\\n  .parquet('file:///dbfs/ml/tmp/assembled_t')\n\n# write val to parquet\nassembled_val.repartition(10) \\\n  .write.mode(\"overwrite\") \\\n  .option(\"parquet.block.size\", 1024 * 1024) \\\n  .parquet('file:///dbfs/ml/tmp/assembled_v')\n\n# write test to parquet\nassembled_test.repartition(10) \\\n  .write.mode(\"overwrite\") \\\n  .option(\"parquet.block.size\", 1024 * 1024) \\\n  .parquet('file:///dbfs/ml/tmp/assembled_test')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write Data to Parquet","showTitle":true,"inputWidgets":{},"nuid":"44a5348f-1404-4848-8018-3e52b5732403"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# get counts of distinct features\nfe = []\nfor v in cat_features:\n  fe.append((v, df.select(v).distinct().count()))\n\n# just get the cardinality\ncat_dims = [x[1] for x in fe]\n\n# find a general value for each\nemb_dims = [(x, min(50, (x + 2) // 2)) for x in cat_dims]\n\n# create embedding dict\nembeddings = {}\nfor i, j in zip(fe, emb_dims):\n  if i[0] not in embeddings:\n    embeddings[i[0]] = j\n\n# set embedding table shape for later use\nembedding_table_shapes = embeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Generate Embedding Data - Categorical Dimensions","showTitle":true,"inputWidgets":{},"nuid":"262a3fa5-31ee-4002-b4c1-277d43fd7428"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# show embedding dims\nembeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"604cd64e-97c7-492f-bfeb-97909f85b225"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[108]: {&#39;DEST_STATE_ABR_cat&#39;: (52, 27),\n &#39;wnd_direction_cat&#39;: (2143, 50),\n &#39;dest_wnd_direction_cat&#39;: (2175, 50),\n &#39;OP_UNIQUE_CARRIER_cat&#39;: (17, 9),\n &#39;OP_CARRIER_cat&#39;: (17, 9),\n &#39;ORIGIN_cat&#39;: (359, 50),\n &#39;ORIGIN_STATE_ABR_cat&#39;: (52, 27),\n &#39;DEST_cat&#39;: (359, 50),\n &#39;cig_code_cat&#39;: (4, 3),\n &#39;cig_cavok_code_cat&#39;: (2, 2),\n &#39;dest_cig_code_cat&#39;: (4, 3),\n &#39;dest_cig_cavok_code_cat&#39;: (2, 2),\n &#39;dest_vis_var_code_cat&#39;: (3, 2),\n &#39;unique_tail_cat&#39;: (8007, 50),\n &#39;DAY_OF_WEEK_cat&#39;: (7, 4),\n &#39;DEP_DEL15_PREV_cat&#39;: (2, 2),\n &#39;MONTH_cat&#39;: (12, 7),\n &#39;QUARTER_cat&#39;: (4, 3),\n &#39;DAY_OF_MONTH_cat&#39;: (31, 16),\n &#39;OP_CARRIER_AIRLINE_ID_cat&#39;: (17, 9),\n &#39;OP_CARRIER_FL_NUM_cat&#39;: (7157, 50),\n &#39;DISTANCE_GROUP_cat&#39;: (11, 6)}</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[108]: {&#39;DEST_STATE_ABR_cat&#39;: (52, 27),\n &#39;wnd_direction_cat&#39;: (2143, 50),\n &#39;dest_wnd_direction_cat&#39;: (2175, 50),\n &#39;OP_UNIQUE_CARRIER_cat&#39;: (17, 9),\n &#39;OP_CARRIER_cat&#39;: (17, 9),\n &#39;ORIGIN_cat&#39;: (359, 50),\n &#39;ORIGIN_STATE_ABR_cat&#39;: (52, 27),\n &#39;DEST_cat&#39;: (359, 50),\n &#39;cig_code_cat&#39;: (4, 3),\n &#39;cig_cavok_code_cat&#39;: (2, 2),\n &#39;dest_cig_code_cat&#39;: (4, 3),\n &#39;dest_cig_cavok_code_cat&#39;: (2, 2),\n &#39;dest_vis_var_code_cat&#39;: (3, 2),\n &#39;unique_tail_cat&#39;: (8007, 50),\n &#39;DAY_OF_WEEK_cat&#39;: (7, 4),\n &#39;DEP_DEL15_PREV_cat&#39;: (2, 2),\n &#39;MONTH_cat&#39;: (12, 7),\n &#39;QUARTER_cat&#39;: (4, 3),\n &#39;DAY_OF_MONTH_cat&#39;: (31, 16),\n &#39;OP_CARRIER_AIRLINE_ID_cat&#39;: (17, 9),\n &#39;OP_CARRIER_FL_NUM_cat&#39;: (7157, 50),\n &#39;DISTANCE_GROUP_cat&#39;: (11, 6)}</div>"]}}],"execution_count":0},{"cell_type":"code","source":["class ConcatenatedEmbeddings(torch.nn.Module):\n    \"\"\"Map multiple categorical variables to concatenated embeddings.\n    Args:\n        embedding_table_shapes: A dictionary mapping column names to\n            (cardinality, embedding_size) tuples.\n        dropout: A float.\n    Inputs:\n        x: An int64 Tensor with shape [batch_size, num_variables].\n    Outputs:\n        A Float Tensor with shape [batch_size, embedding_size_after_concat].\n    \"\"\"\n\n    def __init__(self, embedding_table_shapes, dropout=0.2):\n        super().__init__()\n        self.embedding_layers = torch.nn.ModuleList(\n            [\n                torch.nn.Embedding(cat_size, emb_size)\n                for cat_size, emb_size in embedding_table_shapes.values()\n            ]\n        )\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        x = [layer(x[:, i]) for i, layer in enumerate(self.embedding_layers)]\n        x = torch.cat(x, dim=1)\n        x = self.dropout(x)\n        return x"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Establish Embeddings","showTitle":true,"inputWidgets":{},"nuid":"bd59a82f-ce3e-46f9-8fa7-ecc645520ddd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["class FF_NN(torch.nn.Module):\n    def __init__(self, num_features, num_classes, drop_prob, embedding_table_shapes, num_continuous, emb_dropout):\n        # deep NN with batch norm\n        super(FF_NN, self).__init__()\n        # first hidden layer\n        self.linear_1 = torch.nn.Linear(num_features, num_hidden_1)\n        # batch norm\n        self.linear_1_bn = torch.nn.BatchNorm1d(num_hidden_1)\n        # second hidden layer\n        self.linear_2 = torch.nn.Linear(num_hidden_1, num_hidden_2)\n        # batch norm\n        self.linear_2_bn = torch.nn.BatchNorm1d(num_hidden_2)\n        # third hidden layer\n        self.linear_3 = torch.nn.Linear(num_hidden_2, num_hidden_3)\n        # batch norm\n        self.linear_3_bn = torch.nn.BatchNorm1d(num_hidden_3)        \n        # output layer\n        self.linear_out = torch.nn.Linear(num_hidden_3, num_classes)\n        # dropout\n        self.drop_prob = drop_prob\n        # cat\n        self.initial_cat_layer = ConcatenatedEmbeddings(embedding_table_shapes, dropout=emb_dropout)\n        # cont\n        self.initial_cont_layer = torch.nn.BatchNorm1d(num_continuous)\n \n    # define how and what order model parameters should be used in forward prop.\n    def forward(self, x_cat, x_cont):\n        x_cat = self.initial_cat_layer(x_cat)\n        x_cont = self.initial_cont_layer(x_cont)\n        x = torch.cat([x_cat, x_cont], 1)\n        # run inputs through first layer\n        out = self.linear_1(x)\n        # apply dropout -- doesnt matter position with relu\n        out = F.dropout(out, p=self.drop_prob, training=self.training)   \n        # apply relu\n        out = F.relu(out)\n        # apply batchnorm\n        out = self.linear_1_bn(out)        \n        # run inputs through second layer\n        out = self.linear_2(out)\n        # apply dropout -- doesnt matter position with relu\n        out = F.dropout(out, p=self.drop_prob, training=self.training)        \n        # apply relu\n        out = F.relu(out)\n        # apply batchnorm\n        out = self.linear_2_bn(out)        \n        # run inputs through third layer\n        out = self.linear_3(out)\n        # apply dropout -- doesnt matter position with relu\n        out = F.dropout(out, p=self.drop_prob, training=self.training)        \n        # apply relu\n        out = F.relu(out)\n        # apply batchnorm\n        out = self.linear_3_bn(out)          \n        # run inputs through final classification layer\n        logits = self.linear_out(out)\n        probas = F.log_softmax(logits, dim=1)\n        return logits, probas\n        \n# load the NN model\nnum_hidden_1 = 1000\nnum_hidden_2 = 1000\nnum_hidden_3 = 1000\ndrop_prob = 0.3\nnum_classes = 2\nemb_dropout = 0.1\nnum_continuous = len(cont_features)\nnum_features = sum(emb_size for _, emb_size in embedding_table_shapes.values()) + num_continuous\n \nmodel = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"MLP","showTitle":true,"inputWidgets":{},"nuid":"8fec3263-bf7e-4ea3-b31e-550797fa4877"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# show num features\nprint(num_features)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8ae6207-8ed1-4163-b7ce-eb6223b4e844"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">447\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">447\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# train metrics\ntrain_df_size = train_df.count()\nval_df_size = val_df.count()\ntest_df_size = test_df.count()\nprint(train_df_size)\nprint(val_df_size)\nprint(test_df_size)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Train Metrics","showTitle":true,"inputWidgets":{},"nuid":"a9aeff94-ea60-4bbc-bf9c-d4456a76a4e9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">24554399\n3069321\n3069274\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">24554399\n3069321\n3069274\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["train_df.groupBy('DEP_DEL15').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a9229fc-99c8-4c31-b9a0-01f66737412a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------+--------+\n|DEP_DEL15|   count|\n+---------+--------+\n|      0.0|20080658|\n|      1.0| 4473741|\n+---------+--------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------+\nDEP_DEL15|   count|\n+---------+--------+\n      0.0|20080658|\n      1.0| 4473741|\n+---------+--------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# n samples / n_classes * bincount\nprint(((20080658+4473741) / (2 * np.array([20080658, 4473741]))))\n\nBATCH_SIZE = 100\nNUM_EPOCHS = 7\nweighting = torch.tensor([0.61139428, 2.74428035])  # impose higher costs on misclassified 1s"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f08f359-aa63-4d18-8389-d0bd44da73e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[0.61139428 2.74428035]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[0.61139428 2.74428035]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def _transform_row(batch, cont_cols=['scaledFeatures'], cat_cols=cat_features, label_cols=['DEP_DEL15']):\n    x_cat, x_cont, y = None, None, None\n    x_cat = [batch[col].type(torch.LongTensor) for col in cat_cols]\n    x_cat = torch.stack(x_cat, 1)\n    x_cont = batch['scaledFeatures']\n    y = batch['DEP_DEL15']\n    return x_cat.to(device), x_cont.to(device), y.to(device)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Transform Data","showTitle":true,"inputWidgets":{},"nuid":"e84343b3-5e12-4308-b79e-1a26be5b4c7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["train_loader = BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_t', num_epochs=None,\n                                                   transform_spec=None,\n                                                   shuffle_row_groups=False,\n                                                  workers_count=8), batch_size=4)\nx_cat, x_cont, y = _transform_row(next(iter(train_loader)))\nprint(x_cat, x_cont.squeeze(1), y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Check Data","showTitle":true,"inputWidgets":{},"nuid":"2149efd9-adce-4032-beac-fb74be3e07b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n  column_as_pandas = column.data.chunks[0].to_pandas()\ntensor([[   0,    2,   20,    4,    4,   37,    8,    6,    0,    0,    0,    0,\n            0, 5190,    3,    0,    6,    2,    7,    4,  511,    7],\n        [   2,   28,   18,    0,    0,   22,   14,   13,    0,    0,    0,    0,\n            0, 2703,    2,    1,    7,    1,   15,    0, 4390,    2],\n        [   0,    7,   20,    3,    3,    4,    0,   40,    0,    0,    0,    0,\n            0,  571,    4,    0,    5,    3,   25,    3, 3997,    0],\n        [   9,    4,   16,    2,    2,    9,    7,    5,    0,    0,    0,    0,\n            0, 3409,    2,    0,    7,    1,   20,    2,  191,    8]]) tensor([[ 1.7857,  2.6324,  0.3337, -1.4569, -1.7642, -1.7642, -1.4207, -0.7225,\n          0.3087,  0.4610, -1.2677,  0.3317, -0.2494, -0.1202, -0.1202,  0.8529],\n        [-1.2864,  0.0587,  0.3337, -0.7711, -0.1666, -0.1666,  0.6657, -0.4108,\n         -0.1058,  1.3203, -1.3871, -0.1301,  0.6927,  1.3136,  1.3136, -1.3794],\n        [ 0.6634, -0.7474,  0.3337,  0.4878,  0.2543,  0.2543, -0.2752,  0.5603,\n         -0.7978,  0.0519,  0.8565,  0.3317,  0.2782,  0.0040,  0.0040,  0.8529],\n        [ 1.2839,  1.5653,  0.3337,  1.0609,  0.6274,  0.6274,  0.2566,  0.2905,\n          0.2120,  0.0519,  0.8565,  0.3317,  0.4854, -1.7070, -1.7070,  0.8529]]) tensor([0., 0., 0., 0.], dtype=torch.float64)\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n  column_as_pandas = column.data.chunks[0].to_pandas()\ntensor([[   0,    2,   20,    4,    4,   37,    8,    6,    0,    0,    0,    0,\n            0, 5190,    3,    0,    6,    2,    7,    4,  511,    7],\n        [   2,   28,   18,    0,    0,   22,   14,   13,    0,    0,    0,    0,\n            0, 2703,    2,    1,    7,    1,   15,    0, 4390,    2],\n        [   0,    7,   20,    3,    3,    4,    0,   40,    0,    0,    0,    0,\n            0,  571,    4,    0,    5,    3,   25,    3, 3997,    0],\n        [   9,    4,   16,    2,    2,    9,    7,    5,    0,    0,    0,    0,\n            0, 3409,    2,    0,    7,    1,   20,    2,  191,    8]]) tensor([[ 1.7857,  2.6324,  0.3337, -1.4569, -1.7642, -1.7642, -1.4207, -0.7225,\n          0.3087,  0.4610, -1.2677,  0.3317, -0.2494, -0.1202, -0.1202,  0.8529],\n        [-1.2864,  0.0587,  0.3337, -0.7711, -0.1666, -0.1666,  0.6657, -0.4108,\n         -0.1058,  1.3203, -1.3871, -0.1301,  0.6927,  1.3136,  1.3136, -1.3794],\n        [ 0.6634, -0.7474,  0.3337,  0.4878,  0.2543,  0.2543, -0.2752,  0.5603,\n         -0.7978,  0.0519,  0.8565,  0.3317,  0.2782,  0.0040,  0.0040,  0.8529],\n        [ 1.2839,  1.5653,  0.3337,  1.0609,  0.6274,  0.6274,  0.2566,  0.2905,\n          0.2120,  0.0519,  0.8565,  0.3317,  0.4854, -1.7070, -1.7070,  0.8529]]) tensor([0., 0., 0., 0.], dtype=torch.float64)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_one_epoch(model, optimizer, scheduler, \n                    train_dataloader_iter, steps_per_epoch, epoch, \n                    device):\n  model.train()  # Set model to training mode\n\n  # statistics\n  running_loss = 0.0\n  running_corrects = 0\n  \n  # Iterate over the data for one epoch.\n  for step in range(steps_per_epoch):\n    x_cat, x_cont, labels = _transform_row(next(train_dataloader_iter))\n    \n    # Track history in training\n    with torch.set_grad_enabled(True):\n      # zero the parameter gradients\n      optimizer.zero_grad()\n\n      # forward\n      logits, probas = model(x_cat.long(), x_cont.squeeze(1))\n      _, preds = torch.max(probas, 1)\n      loss = F.cross_entropy(logits, labels.long(), weight=weighting)\n\n      # backward + optimize\n      loss.backward()\n      optimizer.step()\n\n    # statistics\n    running_loss += loss.item() * x_cat.size(0)\n    running_corrects += torch.sum(preds == labels)\n  \n  scheduler.step()\n\n  epoch_loss = running_loss / (steps_per_epoch * BATCH_SIZE)\n  epoch_acc = running_corrects.double() / (steps_per_epoch * BATCH_SIZE)\n\n  print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n  return epoch_loss, epoch_acc\n\ndef evaluate(model, val_dataloader_iter, validation_steps, device, \n             metric_agg_fn=None):\n  model.eval()  # Set model to evaluate mode\n\n  # statistics\n  running_loss = 0.0\n  running_corrects = 0\n\n  # Iterate over all the validation data.\n  for step in range(validation_steps):\n    x_cat, x_cont, labels = _transform_row(next(val_dataloader_iter))\n\n    # Do not track history in evaluation to save memory\n    with torch.set_grad_enabled(False):\n      # forward\n      logits, probas = model(x_cat.long(), x_cont.squeeze(1))\n      _, preds = torch.max(probas, 1)\n      loss = F.cross_entropy(logits, labels.long(), weight=weighting)\n      \n    # statistics\n    running_loss += loss.item()\n    running_corrects += torch.sum(preds == labels)\n   \n  # Average the losses across observations for each minibatch.\n  epoch_loss = running_loss / validation_steps\n  epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n  \n  # metric_agg_fn is used in the distributed training to aggregate the metrics on all workers\n  if metric_agg_fn is not None:\n    epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n    epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n\n  print('Validation Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n  return epoch_loss, epoch_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"One Epoch Loop","showTitle":true,"inputWidgets":{},"nuid":"3a2db91a-faaa-4877-bace-7c1901fb7509"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate(lr=0.016):\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  \n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n  \n  # Only parameters of final layer are being optimized.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1)\n\n  # Decay LR by a factor of 0.1 every 3 epochs\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n  \n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_t',\n                                    num_epochs=None, transform_spec=None, shuffle_row_groups=False,\n                                           workers_count=8), batch_size=BATCH_SIZE) as train_dataloader, \\\n       BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_v',\n                                    num_epochs=None, transform_spec=None, shuffle_row_groups=False,\n                                           workers_count=8), batch_size=BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = train_df_size // BATCH_SIZE\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps = validation_steps = val_df_size // BATCH_SIZE\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      train_loss, train_acc = train_one_epoch(model, optimizer, exp_lr_scheduler, \n                                              train_dataloader_iter, steps_per_epoch, epoch, \n                                              device)\n      \n      val_loss, val_acc = evaluate(model, val_dataloader_iter, validation_steps, device)\n      \n  return val_loss, val_acc\n  \n#loss = train_and_evaluate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Single Worker","showTitle":true,"inputWidgets":{},"nuid":"833484b5-3485-42de-92ff-9b0748cdaf50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def metric_average(val, name):\n  tensor = torch.as_tensor(val)\n  avg_tensor = hvd.allreduce(tensor, name=name)\n  return avg_tensor.item()\n\ndef train_and_evaluate_hvd(lr=0.016):\n  hvd.init()  # Initialize Horovod.\n  \n  # Horovod: pin GPU to local rank.\n  if torch.cuda.is_available():\n    torch.cuda.set_device(hvd.local_rank())\n    device = torch.cuda.current_device()\n  else:\n    device = torch.device(\"cpu\")\n  \n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n\n  # Effective batch size in synchronous distributed training is scaled by the number of workers.\n  # An increase in learning rate compensates for the increased batch size.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr * hvd.size(), momentum=0.9)\n  \n  # Broadcast initial parameters so all workers start with the same parameters.\n  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n  hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n  \n  # Wrap the optimizer with Horovod's DistributedOptimizer.\n  optimizer_hvd = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_hvd, step_size=5, gamma=0.1)\n\n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_t',\n                                    num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size(),\n                                    transform_spec=None, shuffle_row_groups=False, workers_count=8), batch_size=BATCH_SIZE) as train_dataloader, \\\n       BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_v',\n                                    num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size(),\n                                    transform_spec=None, shuffle_row_groups=False, workers_count=8), batch_size=BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = train_df_size // (BATCH_SIZE * hvd.size())\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps =  val_df_size // (BATCH_SIZE * hvd.size())\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      train_loss, train_acc = train_one_epoch(model, optimizer_hvd, exp_lr_scheduler, \n                                              train_dataloader_iter, steps_per_epoch, epoch, \n                                              device)\n      \n      # save checkpoint\n      if hvd.rank() == 0: save_checkpoint(model, optimizer_hvd, epoch)\n      \n      val_loss, val_acc = evaluate(model, val_dataloader_iter, validation_steps,\n                                   device, metric_agg_fn=metric_average)\n      \n  return val_loss, val_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Horovod","showTitle":true,"inputWidgets":{},"nuid":"b07214f2-6300-4ca4-9dfd-b3470c42aa25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["hr = HorovodRunner(np=10)   # This assumes the cluster consists of 10 workers.\nhr.run(train_and_evaluate_hvd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b239eac-9309-4d72-ad17-ae8d8b3ab97f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nThe global names read or written to by the pickled function are {&#39;NUM_EPOCHS&#39;, &#39;hvd&#39;, &#39;val_df_size&#39;, &#39;num_continuous&#39;, &#39;drop_prob&#39;, &#39;range&#39;, &#39;train_df_size&#39;, &#39;evaluate&#39;, &#39;BatchedDataLoader&#39;, &#39;iter&#39;, &#39;torch&#39;, &#39;embeddings&#39;, &#39;FF_NN&#39;, &#39;make_batch_reader&#39;, &#39;save_checkpoint&#39;, &#39;num_features&#39;, &#39;BATCH_SIZE&#39;, &#39;print&#39;, &#39;metric_average&#39;, &#39;emb_dropout&#39;, &#39;train_one_epoch&#39;}.\nThe pickled object size is 9772 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n[1,4]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,6]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,2]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,7]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,3]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,5]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,8]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,9]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,4]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,2]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,6]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,8]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,3]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,5]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,7]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,9]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,2]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,6]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,3]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,4]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stdout&gt;:Epoch 1/7\n[1,1]&lt;stdout&gt;:----------\n[1,7]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,5]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,8]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,9]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,4]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,2]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,5]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,8]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,6]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stdout&gt;:Epoch 1/7[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,3]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stdout&gt;:\n[1,7]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,9]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,4]&lt;stdout&gt;:Epoch 1/7\n[1,4]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 1/7\n[1,5]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Epoch 1/7\n[1,8]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Epoch 1/7[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,6]&lt;stdout&gt;:Epoch 1/7\n[1,6]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Epoch 1/7\n[1,7]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Epoch 1/7\n[1,2]&lt;stdout&gt;:----------\n[1,0]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,0]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,0]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/pytorch.py:336: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729141890/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n[1,0]&lt;stderr&gt;:  row_as_dict[k] = self.transform_fn(v)\n[1,3]&lt;stdout&gt;:Epoch 1/7\n[1,3]&lt;stdout&gt;:----------\n[1,9]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,9]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,9]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/pytorch.py:336: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729141890/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n[1,9]&lt;stderr&gt;:  row_as_dict[k] = self.transform_fn(v)\n[1,6]&lt;stdout&gt;:Train Loss: 0.5889 Acc: 0.7394\n[1,3]&lt;stdout&gt;:Train Loss: 0.5888 Acc: 0.7394\n[1,1]&lt;stdout&gt;:Train Loss: 0.5895 Acc: 0.7389\n[1,8]&lt;stdout&gt;:Train Loss: 0.5900 Acc: 0.7388\n[1,5]&lt;stdout&gt;:Train Loss: 0.5895 Acc: 0.7394\n[1,4]&lt;stdout&gt;:Train Loss: 0.5898 Acc: 0.7395\n[1,2]&lt;stdout&gt;:Train Loss: 0.5886 Acc: 0.7396\n[1,7]&lt;stdout&gt;:Train Loss: 0.5890 Acc: 0.7393\n[1,9]&lt;stdout&gt;:Train Loss: 0.5891 Acc: 0.7394[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Train Loss: 0.5880 Acc: 0.7399[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,6]&lt;stdout&gt;:Epoch 2/7\n[1,6]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 2/7\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462[1,3]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,3]&lt;stdout&gt;:Epoch 2/7\n[1,3]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,2]&lt;stdout&gt;:Epoch 2/7\n[1,2]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,7]&lt;stdout&gt;:Epoch 2/7\n[1,7]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,8]&lt;stdout&gt;:Epoch 2/7\n[1,8]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,5]&lt;stdout&gt;:Epoch 2/7\n[1,4]&lt;stdout&gt;:Epoch 2/7\n[1,4]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 2/7\n[1,0]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Epoch 2/7\n[1,9]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Train Loss: 0.5701 Acc: 0.7495\n[1,1]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7490\n[1,2]&lt;stdout&gt;:Train Loss: 0.5698 Acc: 0.7498\n[1,3]&lt;stdout&gt;:Train Loss: 0.5701 Acc: 0.7496\n[1,4]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7494\n[1,5]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7490\n[1,8]&lt;stdout&gt;:Train Loss: 0.5710 Acc: 0.7492\n[1,7]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7493\n[1,9]&lt;stdout&gt;:Train Loss: 0.5702 Acc: 0.7494[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Train Loss: 0.5692 Acc: 0.7500[1,0]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,9]&lt;stdout&gt;:Epoch 3/7\n[1,9]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,3]&lt;stdout&gt;:Epoch 3/7\n[1,3]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,2]&lt;stdout&gt;:Epoch 3/7\n[1,2]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,6]&lt;stdout&gt;:Epoch 3/7\n[1,6]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,0]&lt;stdout&gt;:Epoch 3/7\n[1,0]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 3/7\n[1,1]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,5]&lt;stdout&gt;:Epoch 3/7\n[1,5]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,7]&lt;stdout&gt;:Epoch 3/7\n[1,7]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,4]&lt;stdout&gt;:Epoch 3/7\n[1,4]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,8]&lt;stdout&gt;:Epoch 3/7\n[1,8]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Train Loss: 0.5664 Acc: 0.7515\n[1,3]&lt;stdout&gt;:Train Loss: 0.5663 Acc: 0.7513\n[1,5]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7509\n[1,1]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7510\n[1,4]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7511\n[1,7]&lt;stdout&gt;:Train Loss: 0.5665 Acc: 0.7513\n[1,8]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7510\n[1,2]&lt;stdout&gt;:Train Loss: 0.5662 Acc: 0.7515\n[1,0]&lt;stdout&gt;:Train Loss: 0.5655 Acc: 0.7517[1,0]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Train Loss: 0.5663 Acc: 0.7513[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437[1,3]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,3]&lt;stdout&gt;:Epoch 4/7\n[1,3]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,6]&lt;stdout&gt;:Epoch 4/7\n[1,6]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 4/7\n[1,1]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,0]&lt;stdout&gt;:Epoch 4/7\n[1,0]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Epoch 4/7\n[1,2]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,7]&lt;stdout&gt;:Epoch 4/7\n[1,7]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 4/7\n[1,5]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Epoch 4/7\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,4]&lt;stdout&gt;:Epoch 4/7\n[1,8]&lt;stdout&gt;:Epoch 4/7\n[1,8]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,6]&lt;stdout&gt;:Train Loss: 0.5642 Acc: 0.7525\n[1,1]&lt;stdout&gt;:Train Loss: 0.5646 Acc: 0.7521\n[1,3]&lt;stdout&gt;:Train Loss: 0.5639 Acc: 0.7524\n[1,2]&lt;stdout&gt;:Train Loss: 0.5639 Acc: 0.7527\n[1,4]&lt;stdout&gt;:Train Loss: 0.5647 Acc: 0.7521\n[1,7]&lt;stdout&gt;:Train Loss: 0.5644 Acc: 0.7524\n[1,5]&lt;stdout&gt;:Train Loss: 0.5648 Acc: 0.7522\n[1,8]&lt;stdout&gt;:Train Loss: 0.5649 Acc: 0.7521\n[1,0]&lt;stdout&gt;:Train Loss: 0.5633 Acc: 0.7528[1,9]&lt;stdout&gt;:Train Loss: 0.5641 Acc: 0.7526\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407[1,4]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,4]&lt;stdout&gt;:Epoch 5/7\n[1,4]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 5/7\n[1,0]&lt;stdout&gt;:----------\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,3]&lt;stdout&gt;:Epoch 5/7\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,3]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,2]&lt;stdout&gt;:Epoch 5/7\n[1,2]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Epoch 5/7\n[1,6]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Epoch 5/7[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,1]&lt;stdout&gt;:Epoch 5/7\n[1,1]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Epoch 5/7\n[1,7]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 5/7\n[1,5]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,8]&lt;stdout&gt;:Epoch 5/7\n[1,8]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Train Loss: 0.5627 Acc: 0.7528\n[1,1]&lt;stdout&gt;:Train Loss: 0.5633 Acc: 0.7527\n[1,3]&lt;stdout&gt;:Train Loss: 0.5626 Acc: 0.7533\n[1,2]&lt;stdout&gt;:Train Loss: 0.5625 Acc: 0.7531\n[1,4]&lt;stdout&gt;:Train Loss: 0.5629 Acc: 0.7531\n[1,5]&lt;stdout&gt;:Train Loss: 0.5634 Acc: 0.7526\n[1,8]&lt;stdout&gt;:Train Loss: 0.5634 Acc: 0.7528\n[1,9]&lt;stdout&gt;:Train Loss: 0.5625 Acc: 0.7532[1,9]&lt;stdout&gt;:\n[1,7]&lt;stdout&gt;:Train Loss: 0.5626 Acc: 0.7529\n[1,0]&lt;stdout&gt;:Train Loss: 0.5618 Acc: 0.7534\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,3]&lt;stdout&gt;:Epoch 6/7\n[1,3]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,5]&lt;stdout&gt;:Epoch 6/7\n[1,5]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,2]&lt;stdout&gt;:Epoch 6/7\n[1,2]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,7]&lt;stdout&gt;:Epoch 6/7\n[1,7]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,4]&lt;stdout&gt;:Epoch 6/7\n[1,4]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,1]&lt;stdout&gt;:Epoch 6/7\n[1,1]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Epoch 6/7\n[1,6]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Epoch 6/7[1,0]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 6/7[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,8]&lt;stdout&gt;:Epoch 6/7\n[1,8]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7544\n[1,3]&lt;stdout&gt;:Train Loss: 0.5591 Acc: 0.7547\n[1,4]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7542\n[1,6]&lt;stdout&gt;:Train Loss: 0.5593 Acc: 0.7546\n[1,2]&lt;stdout&gt;:Train Loss: 0.5591 Acc: 0.7548\n[1,5]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7543\n[1,7]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7544\n[1,0]&lt;stdout&gt;:Train Loss: 0.5583 Acc: 0.7551[1,0]&lt;stdout&gt;:\n[1,8]&lt;stdout&gt;:Train Loss: 0.5600 Acc: 0.7543\n[1,9]&lt;stdout&gt;:Train Loss: 0.5592 Acc: 0.7547\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,6]&lt;stdout&gt;:Epoch 7/7\n[1,6]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 7/7\n[1,1]&lt;stdout&gt;:----------\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,3]&lt;stdout&gt;:Epoch 7/7\n[1,3]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,8]&lt;stdout&gt;:Epoch 7/7\n[1,8]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 7/7\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,4]&lt;stdout&gt;:Epoch 7/7\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,2]&lt;stdout&gt;:Epoch 7/7\n[1,2]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,7]&lt;stdout&gt;:Epoch 7/7\n[1,5]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Epoch 7/7[1,0]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 7/7\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">HorovodRunner will stream all training logs to notebook cell output. If there are too many logs, you\ncan adjust the log level in your train method. Or you can set driver_log_verbosity to\n&#39;log_callback_only&#39; and use a HorovodRunner log  callback on the first worker to get concise\nprogress updates.\nThe global names read or written to by the pickled function are {&#39;NUM_EPOCHS&#39;, &#39;hvd&#39;, &#39;val_df_size&#39;, &#39;num_continuous&#39;, &#39;drop_prob&#39;, &#39;range&#39;, &#39;train_df_size&#39;, &#39;evaluate&#39;, &#39;BatchedDataLoader&#39;, &#39;iter&#39;, &#39;torch&#39;, &#39;embeddings&#39;, &#39;FF_NN&#39;, &#39;make_batch_reader&#39;, &#39;save_checkpoint&#39;, &#39;num_features&#39;, &#39;BATCH_SIZE&#39;, &#39;print&#39;, &#39;metric_average&#39;, &#39;emb_dropout&#39;, &#39;train_one_epoch&#39;}.\nThe pickled object size is 9772 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n[1,4]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,6]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,2]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,7]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,3]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,5]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,8]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,9]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,4]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,2]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,6]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,8]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,3]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,5]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,7]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,9]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,2]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,6]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,3]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,0]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,1]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,4]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,1]&lt;stdout&gt;:Epoch 1/7\n[1,1]&lt;stdout&gt;:----------\n[1,7]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,5]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,8]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,9]&lt;stderr&gt;:Failed loading Unischema from metadata in %s. Assuming the dataset was not created with Petastorm. Will try to construct from native Parquet schema.\n[1,4]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,2]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,5]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,8]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,6]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stdout&gt;:Epoch 1/7[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,3]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,0]&lt;stdout&gt;:\n[1,7]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,9]&lt;stderr&gt;:Recovering rowgroup information for the entire dataset. This can take a long time for datasets with large number of files. If this dataset was generated by Petastorm (i.e. by using &#34;with materialize_dataset(...)&#34;) and you still see this message, this indicates that the materialization did not finish successfully.\n[1,4]&lt;stdout&gt;:Epoch 1/7\n[1,4]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 1/7\n[1,5]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Epoch 1/7\n[1,8]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Epoch 1/7[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,6]&lt;stdout&gt;:Epoch 1/7\n[1,6]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Epoch 1/7\n[1,7]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Epoch 1/7\n[1,2]&lt;stdout&gt;:----------\n[1,0]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,0]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,0]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/pytorch.py:336: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729141890/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n[1,0]&lt;stderr&gt;:  row_as_dict[k] = self.transform_fn(v)\n[1,3]&lt;stdout&gt;:Epoch 1/7\n[1,3]&lt;stdout&gt;:----------\n[1,9]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n[1,9]&lt;stderr&gt;:  column_as_pandas = column.data.chunks[0].to_pandas()\n[1,9]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/petastorm/pytorch.py:336: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729141890/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n[1,9]&lt;stderr&gt;:  row_as_dict[k] = self.transform_fn(v)\n[1,6]&lt;stdout&gt;:Train Loss: 0.5889 Acc: 0.7394\n[1,3]&lt;stdout&gt;:Train Loss: 0.5888 Acc: 0.7394\n[1,1]&lt;stdout&gt;:Train Loss: 0.5895 Acc: 0.7389\n[1,8]&lt;stdout&gt;:Train Loss: 0.5900 Acc: 0.7388\n[1,5]&lt;stdout&gt;:Train Loss: 0.5895 Acc: 0.7394\n[1,4]&lt;stdout&gt;:Train Loss: 0.5898 Acc: 0.7395\n[1,2]&lt;stdout&gt;:Train Loss: 0.5886 Acc: 0.7396\n[1,7]&lt;stdout&gt;:Train Loss: 0.5890 Acc: 0.7393\n[1,9]&lt;stdout&gt;:Train Loss: 0.5891 Acc: 0.7394[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Train Loss: 0.5880 Acc: 0.7399[1,0]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,6]&lt;stdout&gt;:Epoch 2/7\n[1,6]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 2/7\n[1,1]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462[1,3]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,3]&lt;stdout&gt;:Epoch 2/7\n[1,3]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,2]&lt;stdout&gt;:Epoch 2/7\n[1,2]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,7]&lt;stdout&gt;:Epoch 2/7\n[1,7]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,8]&lt;stdout&gt;:Epoch 2/7\n[1,8]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462\n[1,5]&lt;stdout&gt;:Epoch 2/7\n[1,4]&lt;stdout&gt;:Epoch 2/7\n[1,4]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 2/7\n[1,0]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5684 Acc: 0.7462[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Epoch 2/7\n[1,9]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Train Loss: 0.5701 Acc: 0.7495\n[1,1]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7490\n[1,2]&lt;stdout&gt;:Train Loss: 0.5698 Acc: 0.7498\n[1,3]&lt;stdout&gt;:Train Loss: 0.5701 Acc: 0.7496\n[1,4]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7494\n[1,5]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7490\n[1,8]&lt;stdout&gt;:Train Loss: 0.5710 Acc: 0.7492\n[1,7]&lt;stdout&gt;:Train Loss: 0.5705 Acc: 0.7493\n[1,9]&lt;stdout&gt;:Train Loss: 0.5702 Acc: 0.7494[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Train Loss: 0.5692 Acc: 0.7500[1,0]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,9]&lt;stdout&gt;:Epoch 3/7\n[1,9]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,3]&lt;stdout&gt;:Epoch 3/7\n[1,3]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,2]&lt;stdout&gt;:Epoch 3/7\n[1,2]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,6]&lt;stdout&gt;:Epoch 3/7\n[1,6]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,0]&lt;stdout&gt;:Epoch 3/7\n[1,0]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 3/7\n[1,1]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,5]&lt;stdout&gt;:Epoch 3/7\n[1,5]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,7]&lt;stdout&gt;:Epoch 3/7\n[1,7]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,4]&lt;stdout&gt;:Epoch 3/7\n[1,4]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5666 Acc: 0.7451\n[1,8]&lt;stdout&gt;:Epoch 3/7\n[1,8]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Train Loss: 0.5664 Acc: 0.7515\n[1,3]&lt;stdout&gt;:Train Loss: 0.5663 Acc: 0.7513\n[1,5]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7509\n[1,1]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7510\n[1,4]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7511\n[1,7]&lt;stdout&gt;:Train Loss: 0.5665 Acc: 0.7513\n[1,8]&lt;stdout&gt;:Train Loss: 0.5670 Acc: 0.7510\n[1,2]&lt;stdout&gt;:Train Loss: 0.5662 Acc: 0.7515\n[1,0]&lt;stdout&gt;:Train Loss: 0.5655 Acc: 0.7517[1,0]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Train Loss: 0.5663 Acc: 0.7513[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437[1,3]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,3]&lt;stdout&gt;:Epoch 4/7\n[1,3]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,6]&lt;stdout&gt;:Epoch 4/7\n[1,6]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 4/7\n[1,1]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,0]&lt;stdout&gt;:Epoch 4/7\n[1,0]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Epoch 4/7\n[1,2]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,7]&lt;stdout&gt;:Epoch 4/7\n[1,7]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 4/7\n[1,5]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Epoch 4/7\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5655 Acc: 0.7437\n[1,4]&lt;stdout&gt;:Epoch 4/7\n[1,8]&lt;stdout&gt;:Epoch 4/7\n[1,8]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,6]&lt;stdout&gt;:Train Loss: 0.5642 Acc: 0.7525\n[1,1]&lt;stdout&gt;:Train Loss: 0.5646 Acc: 0.7521\n[1,3]&lt;stdout&gt;:Train Loss: 0.5639 Acc: 0.7524\n[1,2]&lt;stdout&gt;:Train Loss: 0.5639 Acc: 0.7527\n[1,4]&lt;stdout&gt;:Train Loss: 0.5647 Acc: 0.7521\n[1,7]&lt;stdout&gt;:Train Loss: 0.5644 Acc: 0.7524\n[1,5]&lt;stdout&gt;:Train Loss: 0.5648 Acc: 0.7522\n[1,8]&lt;stdout&gt;:Train Loss: 0.5649 Acc: 0.7521\n[1,0]&lt;stdout&gt;:Train Loss: 0.5633 Acc: 0.7528[1,9]&lt;stdout&gt;:Train Loss: 0.5641 Acc: 0.7526\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407[1,4]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,4]&lt;stdout&gt;:Epoch 5/7\n[1,4]&lt;stdout&gt;:----------\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 5/7\n[1,0]&lt;stdout&gt;:----------\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,3]&lt;stdout&gt;:Epoch 5/7\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,3]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,2]&lt;stdout&gt;:Epoch 5/7\n[1,2]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Epoch 5/7\n[1,6]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Epoch 5/7[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,1]&lt;stdout&gt;:Epoch 5/7\n[1,1]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Epoch 5/7\n[1,7]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 5/7\n[1,5]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5674 Acc: 0.7407\n[1,8]&lt;stdout&gt;:Epoch 5/7\n[1,8]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Train Loss: 0.5627 Acc: 0.7528\n[1,1]&lt;stdout&gt;:Train Loss: 0.5633 Acc: 0.7527\n[1,3]&lt;stdout&gt;:Train Loss: 0.5626 Acc: 0.7533\n[1,2]&lt;stdout&gt;:Train Loss: 0.5625 Acc: 0.7531\n[1,4]&lt;stdout&gt;:Train Loss: 0.5629 Acc: 0.7531\n[1,5]&lt;stdout&gt;:Train Loss: 0.5634 Acc: 0.7526\n[1,8]&lt;stdout&gt;:Train Loss: 0.5634 Acc: 0.7528\n[1,9]&lt;stdout&gt;:Train Loss: 0.5625 Acc: 0.7532[1,9]&lt;stdout&gt;:\n[1,7]&lt;stdout&gt;:Train Loss: 0.5626 Acc: 0.7529\n[1,0]&lt;stdout&gt;:Train Loss: 0.5618 Acc: 0.7534\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,3]&lt;stdout&gt;:Epoch 6/7\n[1,3]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,5]&lt;stdout&gt;:Epoch 6/7\n[1,5]&lt;stdout&gt;:----------\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,2]&lt;stdout&gt;:Epoch 6/7\n[1,2]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,7]&lt;stdout&gt;:Epoch 6/7\n[1,7]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,4]&lt;stdout&gt;:Epoch 6/7\n[1,4]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,1]&lt;stdout&gt;:Epoch 6/7\n[1,1]&lt;stdout&gt;:----------\n[1,6]&lt;stdout&gt;:Epoch 6/7\n[1,6]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Epoch 6/7[1,0]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 6/7[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5810 Acc: 0.7413\n[1,8]&lt;stdout&gt;:Epoch 6/7\n[1,8]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7544\n[1,3]&lt;stdout&gt;:Train Loss: 0.5591 Acc: 0.7547\n[1,4]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7542\n[1,6]&lt;stdout&gt;:Train Loss: 0.5593 Acc: 0.7546\n[1,2]&lt;stdout&gt;:Train Loss: 0.5591 Acc: 0.7548\n[1,5]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7543\n[1,7]&lt;stdout&gt;:Train Loss: 0.5597 Acc: 0.7544\n[1,0]&lt;stdout&gt;:Train Loss: 0.5583 Acc: 0.7551[1,0]&lt;stdout&gt;:\n[1,8]&lt;stdout&gt;:Train Loss: 0.5600 Acc: 0.7543\n[1,9]&lt;stdout&gt;:Train Loss: 0.5592 Acc: 0.7547\n[1,5]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,1]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,6]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,6]&lt;stdout&gt;:Epoch 7/7\n[1,6]&lt;stdout&gt;:----------\n[1,1]&lt;stdout&gt;:Epoch 7/7\n[1,1]&lt;stdout&gt;:----------\n[1,3]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,3]&lt;stdout&gt;:Epoch 7/7\n[1,3]&lt;stdout&gt;:----------\n[1,8]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,8]&lt;stdout&gt;:Epoch 7/7\n[1,8]&lt;stdout&gt;:----------\n[1,5]&lt;stdout&gt;:Epoch 7/7\n[1,4]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,4]&lt;stdout&gt;:Epoch 7/7\n[1,2]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,2]&lt;stdout&gt;:Epoch 7/7\n[1,2]&lt;stdout&gt;:----------\n[1,4]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324\n[1,7]&lt;stdout&gt;:Epoch 7/7\n[1,5]&lt;stdout&gt;:----------\n[1,7]&lt;stdout&gt;:----------\n[1,9]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:Epoch 7/7[1,0]&lt;stdout&gt;:Validation Loss: 0.5888 Acc: 0.7324[1,9]&lt;stdout&gt;:\n[1,9]&lt;stdout&gt;:----------[1,9]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:Epoch 7/7\n[1,0]&lt;stdout&gt;:----------[1,0]&lt;stdout&gt;:\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Cancelled","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# review checkpoint files\ndisplay(dbutils.fs.ls('dbfs:/ml/horovod_pytorch/take2/PetaFlights'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Review Checkpoint Files","showTitle":true,"inputWidgets":{},"nuid":"a66c3f4e-2c92-4a2a-afdd-9c5adba13062"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-0.pth.tar","checkpoint-0.pth.tar",27828423],["dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-1.pth.tar","checkpoint-1.pth.tar",27828423],["dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-2.pth.tar","checkpoint-2.pth.tar",27828423],["dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-3.pth.tar","checkpoint-3.pth.tar",27828423],["dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-4.pth.tar","checkpoint-4.pth.tar",27828423],["dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-5.pth.tar","checkpoint-5.pth.tar",27828423]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-0.pth.tar</td><td>checkpoint-0.pth.tar</td><td>27828423</td></tr><tr><td>dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-1.pth.tar</td><td>checkpoint-1.pth.tar</td><td>27828423</td></tr><tr><td>dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-2.pth.tar</td><td>checkpoint-2.pth.tar</td><td>27828423</td></tr><tr><td>dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-3.pth.tar</td><td>checkpoint-3.pth.tar</td><td>27828423</td></tr><tr><td>dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-4.pth.tar</td><td>checkpoint-4.pth.tar</td><td>27828423</td></tr><tr><td>dbfs:/ml/horovod_pytorch/take2/PetaFlights/checkpoint-5.pth.tar</td><td>checkpoint-5.pth.tar</td><td>27828423</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["NUM_EPOCHS=1\nBATCH_SIZE=100\n\ndef evaluate(model, val_dataloader_iter, validation_steps, device, \n             metric_agg_fn=None):\n  model.eval()  # Set model to evaluate mode\n\n  # statistics\n  running_loss = 0.0\n  running_corrects = 0\n  \n  # for f1 and other metrics\n  global preds1\n  preds1 = []\n  global labels1\n  labels1 = []\n  \n  # Iterate over all the validation data.\n  for step in range(validation_steps):\n    x_cat, x_cont, labels = _transform_row(next(val_dataloader_iter))\n\n    # Do not track history in evaluation to save memory\n    with torch.set_grad_enabled(False):\n      # forward\n      logits, probas = model(x_cat.long(), x_cont.squeeze(1))\n      _, preds = torch.max(probas, 1)\n      loss = F.cross_entropy(logits, labels.long(), weight=weighting)\n      \n      preds1.append(preds)\n      labels1.append(labels)\n\n    # statistics\n    running_loss += loss.item()\n    running_corrects += torch.sum(preds == labels)\n   \n  # Average the losses across observations for each minibatch.\n  epoch_loss = running_loss / validation_steps\n  epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n  epoch_f1 = f1_score(torch.cat(preds1), torch.cat(labels1), average='weighted')\n  \n  # metric_agg_fn is used in the distributed training to aggregate the metrics on all workers\n  if metric_agg_fn is not None:\n    epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n    epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n\n  print('Test Loss: {:.4f} Acc: {:.4f}, F1: {:.4f}'.format(epoch_loss, epoch_acc, epoch_f1))\n  return epoch_loss, epoch_acc, epoch_f1\n\ndef train_and_evaluate(lr=0.016):\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  \n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n  model.load_state_dict(torch.load('/dbfs/ml/horovod_pytorch/take2/PetaFlights/checkpoint-2.pth.tar')['model'])\n  \n  # Only parameters of final layer are being optimized.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1)\n\n  # Decay LR by a factor of 0.1 every 3 epochs\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n  \n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_test',\n                                    num_epochs=None,\n                                           transform_spec=None,\n                                           shuffle_row_groups=False),\n                         batch_size=BATCH_SIZE) as val_dataloader:\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps = val_df_size // BATCH_SIZE\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      val_loss, val_acc, val_f1 = evaluate(model, val_dataloader_iter, validation_steps, device)\n\n  return val_loss, val_acc, val_f1\n  \nloss, acc, f1 = train_and_evaluate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Single Worker Test Set","showTitle":true,"inputWidgets":{},"nuid":"80c4fef8-2eb4-44f8-88f9-77560ca275b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Epoch 1/1\n----------\n/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n  column_as_pandas = column.data.chunks[0].to_pandas()\nTest Loss: 0.5701 Acc: 0.7760, F1: 0.7604\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Epoch 1/1\n----------\n/databricks/python/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n  column_as_pandas = column.data.chunks[0].to_pandas()\nTest Loss: 0.5701 Acc: 0.7760, F1: 0.7604\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(confusion_matrix(torch.cat(preds1), torch.cat(labels1)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Confusion Matrix","showTitle":true,"inputWidgets":{},"nuid":"9cdc321f-fe5f-4328-bb13-006966f6f1f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[[2076077  220087]\n [ 467514  305622]]\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[[2076077  220087]\n [ 467514  305622]]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(classification_report(torch.cat(preds1), torch.cat(labels1)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Classification Report","showTitle":true,"inputWidgets":{},"nuid":"608a1f55-3167-4ecf-ad5b-615d736dadbd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">              precision    recall  f1-score   support\n\n           0       0.82      0.90      0.86   2296164\n           1       0.58      0.40      0.47    773136\n\n    accuracy                           0.78   3069300\n   macro avg       0.70      0.65      0.66   3069300\nweighted avg       0.76      0.78      0.76   3069300\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              precision    recall  f1-score   support\n\n           0       0.82      0.90      0.86   2296164\n           1       0.58      0.40      0.47    773136\n\n    accuracy                           0.78   3069300\n   macro avg       0.70      0.65      0.66   3069300\nweighted avg       0.76      0.78      0.76   3069300\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\nroc_auc_score(torch.cat(preds1), torch.cat(labels1), average='weighted')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"ROC AUC","showTitle":true,"inputWidgets":{},"nuid":"1cc300fc-3088-4004-85e0-e1a4ae6bfab1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[106]: 0.6497259356338506</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[106]: 0.6497259356338506</div>"]}}],"execution_count":0},{"cell_type":"code","source":["f1_score(torch.cat(preds1), torch.cat(labels1), average='weighted')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"F1 Macro","showTitle":true,"inputWidgets":{},"nuid":"16826291-cc5f-434a-9347-9d19b3081e9e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[107]: 0.7603630154246153</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[107]: 0.7603630154246153</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# intentionally left blank"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3c045fb7-86ba-4e75-b2b1-371e2f95520b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate(lr=0.001, weight_decay=2, batch_size=BATCH_SIZE):\n  hvd.init()\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n  criterion = torch.nn.CrossEntropyLoss()\n\n  # Only parameters of final layer are being optimized.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n\n  # Decay LR by a factor of 0.1 every 7 epochs\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  \n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/tmp/assembled_t',\n                                    num_epochs=None,\n                                    transform_spec=None, shuffle_row_groups=False, workers_count=8,\n                                          cur_shard=hvd.rank(), shard_count=hvd.size()), batch_size=BATCH_SIZE) as train_dataloader, \\\n       BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/tmp/assembled_v',\n                                    num_epochs=None, transform_spec=None, shuffle_row_groups=False, workers_count=8,\n                                          cur_shard=hvd.rank(), shard_count=hvd.size()), batch_size=BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = train_df_size // BATCH_SIZE\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps =  max(1, val_df_size // (BATCH_SIZE))\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      train_loss, train_acc = train_one_epoch(model, optimizer, exp_lr_scheduler, \n                                              train_dataloader_iter, steps_per_epoch, epoch, \n                                              device)\n      val_loss, val_acc, val_f1 = evaluate(model, val_dataloader_iter, validation_steps, device)\n\n  return val_loss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Hyperparameter Search: Distributed Hyperopt","showTitle":true,"inputWidgets":{},"nuid":"aebe6d52-4dad-4bac-8211-4e8c26da1277"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["BATCH_SIZE=100\nNUM_EPOCHS=1\ndef train_fn(lr):\n  loss = train_and_evaluate(lr)\n  return {'loss': loss, 'status': STATUS_OK}\n\nsearch_space = hp.loguniform('lr', -10, -4)\n\nargmin = fmin(\n  fn=train_fn,\n  space=search_space,\n  algo=tpe.suggest,\n  max_evals=1,\n  trials=SparkTrials(parallelism=8))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Hyperopt","showTitle":true,"inputWidgets":{},"nuid":"930b9942-ee50-405c-bee1-e8360385641e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["argmin"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22c92616-bf0f-4e65-adad-7840b13f1109"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0b766ac-c9ce-48c1-aa21-b6207eec01c2"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"petastorm FINAL","dashboards":[],"language":"python","widgets":{},"notebookOrigID":2412568272804102}},"nbformat":4,"nbformat_minor":0}
