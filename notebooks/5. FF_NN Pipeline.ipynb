{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as f\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType, FloatType\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.sql.functions import hour, minute, second, to_timestamp, monotonically_increasing_id, row_number, lit, pow, percent_rank\nfrom pyspark.sql.window import Window\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n\n#from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\nfrom pyspark.ml import Pipeline\n\nfrom petastorm.spark.spark_dataset_converter import _convert_vector\nfrom petastorm.pytorch import DataLoader, BatchedDataLoader\nfrom petastorm import make_batch_reader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer, required\n#from torch.utils.data.distributed import DistributedSampler\n\n\n# for distributed computing\nimport horovod.torch as hvd\nfrom sparkdl import HorovodRunner\nfrom hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK\n\n# set number of cores; db says use less\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"120\")\n\n# enable Arrow support.\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\nspark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"200\")\nspark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\n\n# set device\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6c3761b-a02a-4181-ae65-9db680f76487"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\n# number of workers\nsc._jsc.sc().getExecutorMemoryStatus().size()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ba6dbd5-318a-4c84-9448-87d36573e3f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["PYTORCH_DIR = '/dbfs/ml/horovod_pytorch/take2'\n \nLOG_DIR = os.path.join(PYTORCH_DIR, 'PetaFlights')\nif os.path.exists(LOG_DIR) == False:\n    os.makedirs(LOG_DIR)\n    \ndef save_checkpoint(model, optimizer, epoch):\n  filepath = LOG_DIR + '/checkpoint-{epoch}.pth.tar'.format(epoch=epoch)\n  state = {\n    'model': model.state_dict(),\n    'optimizer': optimizer.state_dict(),\n  }\n  torch.save(state, filepath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Setup Checkpoints","showTitle":true,"inputWidgets":{},"nuid":"c8c4d267-cadc-49c1-afac-a90f2468be99"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# load from view\ndf = spark.sql(\"select * from flights_all_v5\")\n\n# filter\ndf = df.filter(f.col('DEP_DELAY') >= -15)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2dd0bba-b1b5-4bb8-be47-cfb7340667fb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# create unique tail\ndf = df.withColumn('unique_tail', f.concat(f.col(\"TAIL_NUM\"), lit(\" \"), f.col(\"OP_UNIQUE_CARRIER\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3f3ff4c-fa01-44c3-b0ae-d5aad50cfa77"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# create time conditions\nw1 = Window.partitionBy().orderBy('unique_tail', 'departure_time')\nw2 = Window.partitionBy('case_id').orderBy('unique_tail', 'departure_time')\n\ndf = df.withColumn(\"case_id\", f.sum(f.when(~(f.col(\"unique_tail\") == f.lag(\"unique_tail\").over(w1)) | (f.lag(\"DEP_DEL15\",1,0).over(w1) == 1),1).otherwise(0)).over(w1)+1) \\\n    .withColumn('time', f.count('*').over(w2)-1)\n\n# create time polynomial\ndf = df.withColumn('time2', f.pow(f.col(\"time\"), 2))\ndf = df.withColumn('time3', f.pow(f.col(\"time\"), 3))\n\n# sort by date\ndf = df.orderBy('unique_tail', 'departure_time', ascending=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e03d8ba-3d73-4c53-bbf6-1ac12013551e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# target, features\ndf = df.select('DEP_DEL15', \"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height', 'departure_time', 'DEST_STATE_ABR', 'wnd_direction', 'dest_wnd_direction', 'OP_UNIQUE_CARRIER','OP_CARRIER', 'ORIGIN', 'ORIGIN_STATE_ABR', 'DEST', 'cig_code','cig_cavok_code','dest_cig_code','dest_cig_cavok_code','dest_vis_var_code', 'unique_tail', 'DAY_OF_WEEK', 'DEP_DEL15_PREV', 'MONTH', 'QUARTER', 'DAY_OF_MONTH', 'OP_CARRIER_AIRLINE_ID','OP_CARRIER_FL_NUM', 'DISTANCE_GROUP', 'OD_GROUP')\n\n# drop some problematic NAs\ndf = df.na.drop(subset=[\"DEP_DEL15\", 'unique_tail'])\n\n# limit for small batch testing\n#df = df.limit(130459)\n\n# create a variable that will be used to split the data into train/valid/test later\ndf = df.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"departure_time\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af044de7-9cb1-4bbc-85fa-d72af8e39adc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# list of str features\nstr_features = ['DEST_STATE_ABR', 'wnd_direction', 'dest_wnd_direction','OP_UNIQUE_CARRIER','OP_CARRIER', 'ORIGIN', 'ORIGIN_STATE_ABR', 'DEST', 'cig_code','cig_cavok_code','dest_cig_code','dest_cig_cavok_code','dest_vis_var_code', 'unique_tail']\n\n# list of int categorical features\nint_categorical = ['DAY_OF_WEEK', 'DEP_DEL15_PREV', 'MONTH', 'QUARTER', 'DAY_OF_MONTH', 'OP_CARRIER_AIRLINE_ID','OP_CARRIER_FL_NUM', 'DISTANCE_GROUP', 'OD_GROUP']\n\n# create indexers\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_cat\").fit(df) for column in str_features+int_categorical]\n\n# pipeline them\npipeline = Pipeline(stages=indexers)\n\n# transform -- drop str and int features listed above\ndf = pipeline.fit(df).transform(df).drop('DEST_STATE_ABR', 'wnd_direction', 'dest_wnd_direction','OP_UNIQUE_CARRIER','OP_CARRIER', 'ORIGIN', 'ORIGIN_STATE_ABR', 'DEST', 'cig_code','cig_cavok_code','dest_cig_code','dest_cig_cavok_code','dest_vis_var_code', 'unique_tail', 'DAY_OF_WEEK', 'DEP_DEL15_PREV', 'MONTH', 'QUARTER', 'DAY_OF_MONTH', 'OP_CARRIER_AIRLINE_ID','OP_CARRIER_FL_NUM', 'DISTANCE_GROUP')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Convert String and Int. Categorical to Zero-min Categorical","showTitle":true,"inputWidgets":{},"nuid":"143e9c55-ada4-48cf-ad26-b7227775d6e9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cont_features = [\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Continuous Features","showTitle":true,"inputWidgets":{},"nuid":"ab1f8317-971c-42a2-af93-6a98dd586646"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cat_features = ['DEST_STATE_ABR_cat', 'wnd_direction_cat', 'dest_wnd_direction_cat','OP_UNIQUE_CARRIER_cat','OP_CARRIER_cat', 'ORIGIN_cat', 'ORIGIN_STATE_ABR_cat', 'DEST_cat', 'cig_code_cat','cig_cavok_code_cat','dest_cig_code_cat','dest_cig_cavok_code_cat','dest_vis_var_code_cat', 'unique_tail_cat', 'DAY_OF_WEEK_cat', 'DEP_DEL15_PREV_cat', 'MONTH_cat', 'QUARTER_cat', 'DAY_OF_MONTH_cat', 'OP_CARRIER_AIRLINE_ID_cat','OP_CARRIER_FL_NUM_cat', 'DISTANCE_GROUP_cat']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Categorical Features","showTitle":true,"inputWidgets":{},"nuid":"6e453b8f-728f-4b64-a700-eb8d65ef7081"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train_df = df.where(\"rank <= .8\").drop(\"rank\", \"departure_time\")\nval_df = df.where(\"rank > .8 and rank < .9\").drop(\"rank\", \"departure_time\")\ntest_df = df.where(\"rank >= .9\").drop(\"rank\", \"departure_time\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create Time Series Train / Valid Sets","showTitle":true,"inputWidgets":{},"nuid":"216ae702-281f-4a70-99ff-4fc624734bab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# select features\nassembler = VectorAssembler(inputCols=cont_features, outputCol=\"features\")\n\n# create vector train_df\nassembled_train = assembler.transform(train_df).drop(\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height')\n\n# create vector val_df\nassembled_val = assembler.transform(val_df).drop(\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height')\n\n# create vector test_df\nassembled_test = assembler.transform(test_df).drop(\"CRS_DEP_TIME\", \"DISTANCE\", 'vis_distance', 'tmp', 'dew', 'elevation', 'dest_wnd_speed', 'pagerank', 'pagerank_dest', 'wnd_speed', 'cig_height', 'dest_vis_distance', 'dest_tmp', 'dest_dew', 'dest_elevation', 'dest_cig_height')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Assemble Continuous Featuers","showTitle":true,"inputWidgets":{},"nuid":"6e5726e9-b01c-4954-bfb7-96f92b922347"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# scale train\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True).fit(assembled_train)\nassembled_train = scaler.transform(assembled_train).drop('features')\nassembled_train = _convert_vector(assembled_train, 'float32')\n\n# scale val \nassembled_val = scaler.transform(assembled_val).drop('features')\nassembled_val = _convert_vector(assembled_val, 'float32')\n\n# scale test \nassembled_test = scaler.transform(assembled_test).drop('features')\nassembled_test = _convert_vector(assembled_test, 'float32')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Scale Continuous Features","showTitle":true,"inputWidgets":{},"nuid":"8a82cc00-e9c0-4c6c-8b8e-da5ba1d8d11e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# check partition size\nassembled_val.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc6fe934-2344-4f61-907e-bac84fe6cb46"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# write train to parquet; at least to # of workers\n# write to dbfs/ml for extra speed performance\nassembled_train.repartition(10) \\\n  .write.mode(\"overwrite\") \\\n  .option(\"parquet.block.size\", 1024 * 1024) \\\n  .parquet('file:///dbfs/ml/tmp/assembled_t')\n\n# write val to parquet\nassembled_val.repartition(10) \\\n  .write.mode(\"overwrite\") \\\n  .option(\"parquet.block.size\", 1024 * 1024) \\\n  .parquet('file:///dbfs/ml/tmp/assembled_v')\n\n# write test to parquet\nassembled_test.repartition(10) \\\n  .write.mode(\"overwrite\") \\\n  .option(\"parquet.block.size\", 1024 * 1024) \\\n  .parquet('file:///dbfs/ml/tmp/assembled_test')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Write Data to Parquet","showTitle":true,"inputWidgets":{},"nuid":"6fc11afa-d8cb-4443-a4d3-5878e0e05bf9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# get counts of distinct features\nfe = []\nfor v in cat_features:\n  fe.append((v, df.select(v).distinct().count()))\n\n# just get the cardinality\ncat_dims = [x[1] for x in fe]\n\n# find a general value for each\nemb_dims = [(x, min(100, (x + 2) // 2)) for x in cat_dims]\n\n# create embedding dict\nembeddings = {}\nfor i, j in zip(fe, emb_dims):\n  if i[0] not in embeddings:\n    embeddings[i[0]] = j\n\n# set embedding table shape for later use\nembedding_table_shapes = embeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Generate Embedding Data - Categorical Dimensions","showTitle":true,"inputWidgets":{},"nuid":"c87bb15d-6364-4d80-8499-71178bececf5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# show embedding dims\nembeddings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a026333-1895-4fef-ab21-f73a48fd8989"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class ConcatenatedEmbeddings(torch.nn.Module):\n    \"\"\"Map multiple categorical variables to concatenated embeddings.\n    Args:\n        embedding_table_shapes: A dictionary mapping column names to\n            (cardinality, embedding_size) tuples.\n        dropout: A float.\n    Inputs:\n        x: An int64 Tensor with shape [batch_size, num_variables].\n    Outputs:\n        A Float Tensor with shape [batch_size, embedding_size_after_concat].\n    \"\"\"\n\n    def __init__(self, embedding_table_shapes, dropout=0.2):\n        super().__init__()\n        self.embedding_layers = torch.nn.ModuleList(\n            [\n                torch.nn.Embedding(cat_size, emb_size)\n                for cat_size, emb_size in embedding_table_shapes.values()\n            ]\n        )\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        x = [layer(x[:, i]) for i, layer in enumerate(self.embedding_layers)]\n        x = torch.cat(x, dim=1)\n        x = self.dropout(x)\n        return x"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Establish Embeddings","showTitle":true,"inputWidgets":{},"nuid":"542fc8e7-115f-48dd-95cd-afc8473a56b2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class FF_NN(torch.nn.Module):\n    def __init__(self, num_features, num_classes, drop_prob, embedding_table_shapes, num_continuous, emb_dropout):\n        # deep NN with batch norm\n        super(FF_NN, self).__init__()\n        # first hidden layer\n        self.linear_1 = torch.nn.Linear(num_features, num_hidden_1)\n        # batch norm\n        self.linear_1_bn = torch.nn.BatchNorm1d(num_hidden_1)\n        # second hidden layer\n        self.linear_2 = torch.nn.Linear(num_hidden_1, num_hidden_2)\n        # batch norm\n        self.linear_2_bn = torch.nn.BatchNorm1d(num_hidden_2)\n        # output layer\n        self.linear_out = torch.nn.Linear(num_hidden_2, num_classes)\n        # dropout\n        self.drop_prob = drop_prob\n        # cat\n        self.initial_cat_layer = ConcatenatedEmbeddings(embedding_table_shapes, dropout=emb_dropout)\n        # cont\n        self.initial_cont_layer = torch.nn.BatchNorm1d(num_continuous)\n        # emb size\n        embedding_size = sum(emb_size for _, emb_size in embedding_table_shapes.values())\n \n    # define how and what order model parameters should be used in forward prop.\n    def forward(self, x_cat, x_cont):\n        x_cat = self.initial_cat_layer(x_cat)\n        x_cont = self.initial_cont_layer(x_cont)\n        x = torch.cat([x_cat, x_cont], 1)\n        # run inputs through first layer\n        out = self.linear_1(x)\n        # apply dropout -- doesnt matter position with relu\n        out = F.dropout(out, p=self.drop_prob, training=self.training)   \n        # apply relu\n        out = F.relu(out)\n        # apply batchnorm\n        out = self.linear_1_bn(out)        \n        # run inputs through second layer\n        out = self.linear_2(out)\n        # apply dropout -- doesnt matter position with relu\n        out = F.dropout(out, p=self.drop_prob, training=self.training)        \n        # apply relu\n        out = F.relu(out)\n        # apply batchnorm\n        out = self.linear_2_bn(out)        \n        # run inputs through final classification layer\n        logits = self.linear_out(out)\n        probas = F.log_softmax(logits, dim=1)\n        return logits, probas\n        \n# load the NN model\nnum_hidden_1 = 1000\nnum_hidden_2 = 750\ndrop_prob = 0.3\nnum_classes = 2\nemb_dropout = 0.1\nnum_continuous = len(cont_features)\nnum_features = sum(emb_size for _, emb_size in embedding_table_shapes.values()) + num_continuous\n \nmodel = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"MLP","showTitle":true,"inputWidgets":{},"nuid":"435057f2-2e19-4450-8a8c-6631374ecd9f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# show num features\nprint(num_features)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b4f9ef6-bdd3-4d57-bb47-b7c648376c72"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# train metrics\ntrain_df_size = train_df.count()\nval_df_size = val_df.count()\ntest_df_size = test_df.count()\nprint(train_df_size)\nprint(val_df_size)\nprint(test_df_size)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Train Metrics","showTitle":true,"inputWidgets":{},"nuid":"8f244b37-bdbc-43b6-a0c3-0ca3a86bdc21"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train_df.groupBy('DEP_DEL15').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf7dd4e1-cdfe-4d92-9329-1d6ce0a4a326"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# n samples / n_classes * bincount\nprint(((20080658+4473741) / (2 * np.array([20080658, 4473741]))))\n\nBATCH_SIZE = 100\nNUM_EPOCHS = 5\nweighting = torch.tensor([0.61139428, 2.74428035])  # impose higher costs on misclassified 1s"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c855269d-c42e-40bf-a0ed-35b2d2ac071b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def _transform_row(batch, cont_cols=['scaledFeatures'], cat_cols=cat_features, label_cols=['DEP_DEL15']):\n    x_cat, x_cont, y = None, None, None\n    x_cat = [batch[col].type(torch.LongTensor) for col in cat_cols]\n    x_cat = torch.stack(x_cat, 1)\n    x_cont = batch['scaledFeatures']\n    y = batch['DEP_DEL15']\n    return x_cat.to(device), x_cont.to(device), y.to(device)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Transform Data","showTitle":true,"inputWidgets":{},"nuid":"1813bef1-a088-40bf-bf40-b4310468027e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train_loader = BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_t', num_epochs=None,\n                                                   transform_spec=None,\n                                                   shuffle_row_groups=False,\n                                                  workers_count=8), batch_size=4)\nx_cat, x_cont, y = _transform_row(next(iter(train_loader)))\nprint(x_cat, x_cont.squeeze(1), y)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Check Data","showTitle":true,"inputWidgets":{},"nuid":"6dcb6d67-d5c7-4805-8a8b-9823ebcaff06"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def train_one_epoch(model, optimizer, scheduler, \n                    train_dataloader_iter, steps_per_epoch, epoch, \n                    device):\n  model.train()  # Set model to training mode\n\n  # statistics\n  running_loss = 0.0\n  running_corrects = 0\n  \n  # Iterate over the data for one epoch.\n  for step in range(steps_per_epoch):\n    x_cat, x_cont, labels = _transform_row(next(train_dataloader_iter))\n    \n    # Track history in training\n    with torch.set_grad_enabled(True):\n      # zero the parameter gradients\n      optimizer.zero_grad()\n\n      # forward\n      logits, probas = model(x_cat.long(), x_cont.squeeze(1))\n      _, preds = torch.max(probas, 1)\n      loss = F.cross_entropy(logits, labels.long(), weight=weighting)\n\n      # backward + optimize\n      loss.backward()\n      optimizer.step()\n\n    # statistics\n    running_loss += loss.item() * x_cat.size(0)\n    running_corrects += torch.sum(preds == labels)\n  \n  scheduler.step()\n\n  epoch_loss = running_loss / (steps_per_epoch * BATCH_SIZE)\n  epoch_acc = running_corrects.double() / (steps_per_epoch * BATCH_SIZE)\n\n  print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n  return epoch_loss, epoch_acc\n\ndef evaluate(model, val_dataloader_iter, validation_steps, device, \n             metric_agg_fn=None):\n  model.eval()  # Set model to evaluate mode\n\n  # statistics\n  running_loss = 0.0\n  running_corrects = 0\n\n  # Iterate over all the validation data.\n  for step in range(validation_steps):\n    x_cat, x_cont, labels = _transform_row(next(val_dataloader_iter))\n\n    # Do not track history in evaluation to save memory\n    with torch.set_grad_enabled(False):\n      # forward\n      logits, probas = model(x_cat.long(), x_cont.squeeze(1))\n      _, preds = torch.max(probas, 1)\n      loss = F.cross_entropy(logits, labels.long(), weight=weighting)\n      \n    # statistics\n    running_loss += loss.item()\n    running_corrects += torch.sum(preds == labels)\n   \n  # Average the losses across observations for each minibatch.\n  epoch_loss = running_loss / validation_steps\n  epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n  \n  # metric_agg_fn is used in the distributed training to aggregate the metrics on all workers\n  if metric_agg_fn is not None:\n    epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n    epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n\n  print('Validation Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n  return epoch_loss, epoch_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"One Epoch Loop","showTitle":true,"inputWidgets":{},"nuid":"801e665c-5253-40a1-97f5-dbb64484db78"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate(lr=0.016):\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  \n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n  \n  # Only parameters of final layer are being optimized.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5, weight_decay=1)\n\n  # Decay LR by a factor of 0.1 every 3 epochs\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n  \n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_t',\n                                    num_epochs=None, transform_spec=None, shuffle_row_groups=False,\n                                           workers_count=8), batch_size=BATCH_SIZE) as train_dataloader, \\\n       BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_v',\n                                    num_epochs=None, transform_spec=None, shuffle_row_groups=False,\n                                           workers_count=8), batch_size=BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = train_df_size // BATCH_SIZE\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps = validation_steps = val_df_size // BATCH_SIZE\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      train_loss, train_acc = train_one_epoch(model, optimizer, exp_lr_scheduler, \n                                              train_dataloader_iter, steps_per_epoch, epoch, \n                                              device)\n      \n      val_loss, val_acc = evaluate(model, val_dataloader_iter, validation_steps, device)\n      \n  return val_loss, val_acc\n  \n#loss = train_and_evaluate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Single Worker","showTitle":true,"inputWidgets":{},"nuid":"04f824c3-d3d5-482d-af4e-abeb45362a11"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def metric_average(val, name):\n  tensor = torch.as_tensor(val)\n  avg_tensor = hvd.allreduce(tensor, name=name)\n  return avg_tensor.item()\n\ndef train_and_evaluate_hvd(lr=0.016):\n  hvd.init()  # Initialize Horovod.\n  \n  # Horovod: pin GPU to local rank.\n  if torch.cuda.is_available():\n    torch.cuda.set_device(hvd.local_rank())\n    device = torch.cuda.current_device()\n  else:\n    device = torch.device(\"cpu\")\n  \n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n\n  # Effective batch size in synchronous distributed training is scaled by the number of workers.\n  # An increase in learning rate compensates for the increased batch size.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr * hvd.size(), momentum=0.5)\n  \n  # Broadcast initial parameters so all workers start with the same parameters.\n  hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n  hvd.broadcast_optimizer_state(optimizer, root_rank=0)\n  \n  # Wrap the optimizer with Horovod's DistributedOptimizer.\n  optimizer_hvd = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_hvd, step_size=5, gamma=0.1)\n\n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_t',\n                                    num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size(),\n                                    transform_spec=None, shuffle_row_groups=False, workers_count=8), batch_size=BATCH_SIZE) as train_dataloader, \\\n       BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_v',\n                                    num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size(),\n                                    transform_spec=None, shuffle_row_groups=False, workers_count=8), batch_size=BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = train_df_size // (BATCH_SIZE * hvd.size())\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps =  val_df_size // (BATCH_SIZE * hvd.size())\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      train_loss, train_acc = train_one_epoch(model, optimizer_hvd, exp_lr_scheduler, \n                                              train_dataloader_iter, steps_per_epoch, epoch, \n                                              device)\n      \n      # save checkpoint\n      if hvd.rank() == 0: save_checkpoint(model, optimizer_hvd, epoch)\n      \n      val_loss, val_acc = evaluate(model, val_dataloader_iter, validation_steps,\n                                   device, metric_agg_fn=metric_average)\n      \n  return val_loss, val_acc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Horovod","showTitle":true,"inputWidgets":{},"nuid":"d9124327-ff95-4d4d-8634-fad9ce66fd4b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["hr = HorovodRunner(np=10)   # This assumes the cluster consists of 10 workers.\nhr.run(train_and_evaluate_hvd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f43bc52b-0989-407a-8e92-0710c5a8a17a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# review checkpoint files\ndisplay(dbutils.fs.ls('dbfs:/ml/horovod_pytorch/take2/PetaFlights'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Review Checkpoint Files","showTitle":true,"inputWidgets":{},"nuid":"d984c4c8-045c-4018-88ba-51983198adca"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["NUM_EPOCHS=1\nBATCH_SIZE=100\n\ndef evaluate(model, val_dataloader_iter, validation_steps, device, \n             metric_agg_fn=None):\n  model.eval()  # Set model to evaluate mode\n\n  # statistics\n  running_loss = 0.0\n  running_corrects = 0\n  \n  # for f1 and other metrics\n  global preds1\n  preds1 = []\n  global labels1\n  labels1 = []\n  \n  # Iterate over all the validation data.\n  for step in range(validation_steps):\n    x_cat, x_cont, labels = _transform_row(next(val_dataloader_iter))\n\n    # Do not track history in evaluation to save memory\n    with torch.set_grad_enabled(False):\n      # forward\n      logits, probas = model(x_cat.long(), x_cont.squeeze(1))\n      _, preds = torch.max(probas, 1)\n      loss = F.cross_entropy(logits, labels.long(), weight=weighting)\n      \n      preds1.append(preds)\n      labels1.append(labels)\n\n    # statistics\n    running_loss += loss.item()\n    running_corrects += torch.sum(preds == labels)\n   \n  # Average the losses across observations for each minibatch.\n  epoch_loss = running_loss / validation_steps\n  epoch_acc = running_corrects.double() / (validation_steps * BATCH_SIZE)\n  epoch_f1 = f1_score(torch.cat(preds1), torch.cat(labels1), average='weighted')\n  \n  # metric_agg_fn is used in the distributed training to aggregate the metrics on all workers\n  if metric_agg_fn is not None:\n    epoch_loss = metric_agg_fn(epoch_loss, 'avg_loss')\n    epoch_acc = metric_agg_fn(epoch_acc, 'avg_acc')\n\n  print('Test Loss: {:.4f} Acc: {:.4f}, F1: {:.4f}'.format(epoch_loss, epoch_acc, epoch_f1))\n  return epoch_loss, epoch_acc, epoch_f1\n\ndef train_and_evaluate(lr=0.016):\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  \n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n  model.load_state_dict(torch.load('/dbfs/ml/horovod_pytorch/take2/PetaFlights/checkpoint-4.pth.tar')['model'])\n  \n  # Only parameters of final layer are being optimized.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.5, weight_decay=1)\n\n  # Decay LR by a factor of 0.1 every 3 epochs\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n  \n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/ml/tmp/assembled_test',\n                                    num_epochs=None,\n                                           transform_spec=None,\n                                           shuffle_row_groups=False),\n                         batch_size=BATCH_SIZE) as val_dataloader:\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps = val_df_size // BATCH_SIZE\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      val_loss, val_acc, val_f1 = evaluate(model, val_dataloader_iter, validation_steps, device)\n\n  return val_loss, val_acc, val_f1\n  \nloss, acc, f1 = train_and_evaluate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Single Worker Test Set","showTitle":true,"inputWidgets":{},"nuid":"c0cc8ecc-87e7-44fa-a2c3-fade899eb885"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(confusion_matrix(torch.cat(preds1), torch.cat(labels1)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Confusion Matrix","showTitle":true,"inputWidgets":{},"nuid":"61660462-4dea-4d08-9442-0c0f82c7c372"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(classification_report(torch.cat(preds1), torch.cat(labels1)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Classification Report","showTitle":true,"inputWidgets":{},"nuid":"937c8359-47b1-4b3d-9785-1472565a8b98"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\nroc_auc_score(torch.cat(preds1), torch.cat(labels1), average='weighted')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"ROC AUC","showTitle":true,"inputWidgets":{},"nuid":"8d179083-0feb-44da-99df-107f8a34320c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# intentionally left blank"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bd1777e-80b2-43ad-bb6f-ce18d8e6aaab"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def train_and_evaluate(lr=0.001, weight_decay=2, batch_size=BATCH_SIZE):\n  hvd.init()\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  model = FF_NN(num_features=num_features, num_classes=2, drop_prob=drop_prob, embedding_table_shapes=embeddings, num_continuous=num_continuous, emb_dropout=emb_dropout)\n  criterion = torch.nn.CrossEntropyLoss()\n\n  # Only parameters of final layer are being optimized.\n  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n\n  # Decay LR by a factor of 0.1 every 7 epochs\n  exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  \n  with BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/tmp/assembled_t',\n                                    num_epochs=None,\n                                    transform_spec=None, shuffle_row_groups=False, workers_count=8,\n                                          cur_shard=hvd.rank(), shard_count=hvd.size()), batch_size=BATCH_SIZE) as train_dataloader, \\\n       BatchedDataLoader(make_batch_reader(dataset_url_or_urls='file:///dbfs/tmp/assembled_v',\n                                    num_epochs=None, transform_spec=None, shuffle_row_groups=False, workers_count=8,\n                                          cur_shard=hvd.rank(), shard_count=hvd.size()), batch_size=BATCH_SIZE) as val_dataloader:\n    \n    train_dataloader_iter = iter(train_dataloader)\n    steps_per_epoch = train_df_size // BATCH_SIZE\n    \n    val_dataloader_iter = iter(val_dataloader)\n    validation_steps =  max(1, val_df_size // (BATCH_SIZE))\n    \n    for epoch in range(NUM_EPOCHS):\n      print('Epoch {}/{}'.format(epoch + 1, NUM_EPOCHS))\n      print('-' * 10)\n\n      train_loss, train_acc = train_one_epoch(model, optimizer, exp_lr_scheduler, \n                                              train_dataloader_iter, steps_per_epoch, epoch, \n                                              device)\n      val_loss, val_acc, val_f1 = evaluate(model, val_dataloader_iter, validation_steps, device)\n\n  return val_loss"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Hyperparameter Search: Distributed Hyperopt","showTitle":true,"inputWidgets":{},"nuid":"8c0c0875-f23c-475c-9dc7-dfdb8c7fb6e5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["BATCH_SIZE=100\nNUM_EPOCHS=1\ndef train_fn(lr):\n  loss = train_and_evaluate(lr)\n  return {'loss': loss, 'status': STATUS_OK}\n\nsearch_space = hp.loguniform('lr', -10, -4)\n\nargmin = fmin(\n  fn=train_fn,\n  space=search_space,\n  algo=tpe.suggest,\n  max_evals=1,\n  trials=SparkTrials(parallelism=8))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Hyperopt","showTitle":true,"inputWidgets":{},"nuid":"c48a69ea-d220-4b8a-aa38-ce0c5b0efd80"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["argmin"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29c00023-d543-4476-b257-efadf5c13a39"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0c91f91-e5a8-402f-b531-3935e2374cb9"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"5. FF_NN Pipeline","dashboards":[],"language":"python","widgets":{},"notebookOrigID":365095366706017}},"nbformat":4,"nbformat_minor":0}
